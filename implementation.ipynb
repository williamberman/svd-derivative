{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b819698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 14:04:08.880529: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-09-01 14:04:08.897882: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import jax\n",
    "from jax import random\n",
    "import torch\n",
    "import torch.autograd.functional as TF\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "torch_jvp = TF.jvp\n",
    "torch_vjp = TF.vjp\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05bc7a9",
   "metadata": {},
   "source": [
    "## Cases\n",
    "$$\n",
    "\\newcommand{\\tr}{{\\mathrm{tr}}}\n",
    "\\newcommand{\\d}{{\\mathrm{d}}}\n",
    "\\newcommand{\\A}{{\\mathbf{A}}}\n",
    "\\newcommand{\\U}{{\\mathbf{U}}}\n",
    "\\newcommand{\\S}{{\\mathbf{S}}}\n",
    "\\newcommand{\\V}{{\\mathbf{V}}}\n",
    "\\newcommand{\\F}{{\\mathbf{F}}}\n",
    "\\newcommand{\\I}{{\\mathbf{I}}}\n",
    "\\newcommand{\\P}{{\\mathbf{P}}}\n",
    "\\newcommand{\\D}{{\\mathbf{D}}}\n",
    "\\newcommand{\\dA}{{\\d\\A}}\n",
    "\\newcommand{\\dU}{{\\d\\U}}\n",
    "\\newcommand{\\dS}{{\\d\\S}}\n",
    "\\newcommand{\\dV}{{\\d\\V}}\n",
    "\\newcommand{\\dP}{{\\d\\P}}\n",
    "\\newcommand{\\dD}{{\\d\\D}}\n",
    "\\newcommand{\\Ut}{{\\U^{\\top}}}\n",
    "\\newcommand{\\Vt}{{\\V^{\\top}}}\n",
    "\\newcommand{\\Vh}{{\\V^{H}}}\n",
    "\\newcommand{\\dAt}{{\\dA^{\\top}}}\n",
    "\\newcommand{\\dVt}{{\\dV^{\\top}}}\n",
    "\\newcommand{\\gA}{{\\overline{\\A}}}\n",
    "\\newcommand{\\gAt}{{\\gA^{\\top}}}\n",
    "\\newcommand{\\gU}{{\\overline{\\U}}}\n",
    "\\newcommand{\\gUt}{{\\gU^{\\top}}}\n",
    "\\newcommand{\\gS}{{\\overline{\\S}}}\n",
    "\\newcommand{\\gSt}{{\\gS^{\\top}}}\n",
    "\\newcommand{\\gV}{{\\overline{\\V}}}\n",
    "\\newcommand{\\gVt}{{\\gV^{\\top}}}\n",
    "$$\n",
    "The derivative of the SVD operation is determined by\n",
    "\n",
    "1. Computing the full SVD vs the \"thin\"/\"partial\" SVD\n",
    "2. complex vs real inputs\n",
    "3. Computing the complete factorization $\\U\\S\\Vh$ vs computing only the singular values $\\S$\n",
    "\n",
    "Computing only the singular values does not change the differential or gradient formulas and can be considered a side case of the full factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac90ffc",
   "metadata": {},
   "source": [
    "## Real valued partial SVD\n",
    "\n",
    "Reference: https://j-towns.github.io/papers/svd-derivative.pdf\n",
    "\n",
    "### Forward mode\n",
    "\n",
    "The differential formulas $\\dU$, $\\dS$, and $\\dV$ in terms of $\\dA$, $\\U$, $\\S$, and $\\V$ are found from the chain rule.\n",
    "\n",
    "##### Chain rule\n",
    "\n",
    "$\\dA = \\dU \\S \\Vt + \\U \\dS \\Vt + \\U \\S \\dVt$\n",
    "\n",
    "TODO(will) - Is this really the chain rule? Deriving this is a little funky because you end up taking partial derivatives with respect to matrices which I'm not sure how to define. The rule for expressing the differential and the product is in 2.1/2.2 https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "\n",
    "##### Differential formulas\n",
    "\n",
    "$\\dU = \\U ( \\F \\circ [\\Ut \\dA \\V \\S + \\S \\Vt \\dAt \\U] ) + (\\I_m - \\U \\Ut ) \\dA \\V \\S^{-1}$\n",
    "\n",
    "$\\dS = \\I_k \\circ [\\Ut \\dA \\V]$\n",
    "\n",
    "$\\dV = \\V (\\F \\circ [\\S \\Ut \\dA \\V + \\Vt \\dAt \\U \\S]) + (\\I_n - \\V \\Vt) \\dAt \\U \\S^{-1}$\n",
    "\n",
    "where\n",
    "\n",
    "$F_{ij} = \\frac{1}{s_j^2 - s_i^2}, i \\neq j$\n",
    "\n",
    "$F_{ij} = 0$ otherwise\n",
    "\n",
    "$k$ is the rank of $\\A$\n",
    "\n",
    "$\\U$ is $m \\times k$\n",
    "\n",
    "$\\S$ is $k \\times k$\n",
    "\n",
    "$\\Vt$ is $k \\times n$\n",
    "\n",
    "##### Numerical instability\n",
    "Both $\\S^{-1}$ and $\\F$ cause numerical instability in $\\dU$ and $\\dV$.  \n",
    "\n",
    "$\\S^{-1}$ causes numerical instability with zero or very small singular values. The inverse of a diagonal matrix is the element wise reciprocals of the inverting matrix's diagonal. Zero singular values will cause $\\S^{-1}$ to have infinite elements. Very small singular values will cause $\\S^{-1}$ to have very large floating point elements.\n",
    "\n",
    "$\\F$ divides by the difference of all pairs of singular values. Repeated singular values will cause infinite elements of $\\F$. Close singular values will cause very large floating point elements of $\\F$.  \n",
    "\n",
    "##### Computing only singular values\n",
    "Only compute the differential for $\\S$. $\\dU$ and $\\dV$ are ignored. Because $\\dU$ and $\\dV$ are not computed, $\\F$ and $\\S^{-1}$ are not needed, so there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56584cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_partial(A, dA):    \n",
    "    U, S_vals, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=False)\n",
    "\n",
    "    S = jnp.diag(S_vals)\n",
    "    Ut = U.T\n",
    "    V = Vt.T\n",
    "    dAt = dA.T\n",
    "\n",
    "    k = S.shape[0]\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "\n",
    "    I_k = jnp.eye(k)\n",
    "    I_m = jnp.eye(m)\n",
    "    I_n = jnp.eye(n)\n",
    "\n",
    "    S_inv = jnp.linalg.inv(S)\n",
    "\n",
    "    F_ = F(S_vals, k)\n",
    "\n",
    "    dU = U @ (F_ * ((Ut @ dA @ V @ S) + (S @ Vt @ dAt @ U))) + ((I_m - U @ Ut) @ dA @ V @ S_inv)\n",
    "    \n",
    "    # Note that the `I_k *` is extraneous. It zeros out the rest of the matrix besides the diagonal.\n",
    "    # We only return `dS_vals` which takes only the diagonal of `dS` anyway. \n",
    "    dS = I_k * (Ut @ dA @ V)\n",
    "    dS_vals = jnp.diagonal(dS)\n",
    "    \n",
    "    dV = V @ (F_ * (S @ Ut @ dA @ V + Vt @ dAt @ U @ S)) + (I_n - V @ Vt) @ dAt @ U @ S_inv\n",
    "    \n",
    "    return (U, S_vals, Vt), (dU, dS_vals, dV.T)\n",
    "\n",
    "def F(S_vals, k):\n",
    "    F_i_j = lambda i, j: lax.cond(i == j, lambda: 0., lambda: 1 / (S_vals[j]**2 - S_vals[i]**2))\n",
    "    F_fun = jax.vmap(jax.vmap(F_i_j, (None, 0)), (0, None))\n",
    "\n",
    "    indices = jnp.arange(k)\n",
    "    F_ = F_fun(indices, indices)\n",
    "    \n",
    "    return F_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b00e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_svd_uv(a, b):\n",
    "    (U, S, Vt), (dU, dS, dVt) = a\n",
    "    (U_, S_, Vt_), (dU_, dS_, dVt_) = b\n",
    "    \n",
    "    assert_allclose(U, U_)\n",
    "    assert_allclose(S, S_)\n",
    "    assert_allclose(Vt, Vt_)\n",
    "    assert_allclose(dU, dU_)\n",
    "    assert_allclose(dS, dS_)\n",
    "    assert_allclose(dVt, dVt_)\n",
    "\n",
    "def check_allclose(l, r):\n",
    "    return jnp.allclose(jnp.array(l), jnp.array(r), atol=1e-04)\n",
    "\n",
    "def assert_allclose(l, r):\n",
    "    assert(check_allclose(l, r))\n",
    "        \n",
    "def jax_real_valued_partial(A, dA):\n",
    "    A = jnp.array(A)\n",
    "    dA = jnp.array(dA)\n",
    "    \n",
    "    return jax.jvp(\n",
    "        lambda A: jnp.linalg.svd(A, compute_uv=True, full_matrices=False), \n",
    "        (A,), \n",
    "        (dA,)\n",
    "    )\n",
    "\n",
    "def pytorch_real_valued_partial(A, dA):\n",
    "    A = torch.tensor(A)\n",
    "    dA = torch.tensor(dA)\n",
    "    \n",
    "    return torch_jvp(lambda A: torch.linalg.svd(A, full_matrices=False), A, dA)\n",
    "    \n",
    "def check_real_valued_partial(A, dA):\n",
    "    jax_res = jax_real_valued_partial(A, dA)\n",
    "    torch_res = pytorch_real_valued_partial(A, dA)\n",
    "    res = svd_jvp_real_valued_partial(jnp.array(A), jnp.array(dA))\n",
    "    \n",
    "    # example == jax == torch\n",
    "    assert_svd_uv(jax_res, torch_res)\n",
    "    assert_svd_uv(jax_res, res)\n",
    "    \n",
    "    return jax_res, torch_res, res\n",
    "\n",
    "def check_real_valued_partial_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "    dA = jnp.ones_like(A)\n",
    "            \n",
    "    A = A.tolist()\n",
    "    dA = dA.tolist()\n",
    "    \n",
    "    return A, dA\n",
    "  \n",
    "def check_real_valued_partials():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_partial_args(subkey, m, n)\n",
    "            check_real_valued_partial(*args)\n",
    "\n",
    "check_real_valued_partials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc7a36",
   "metadata": {},
   "source": [
    "### Reverse mode\n",
    "\n",
    "##### Chain rule\n",
    "$$\n",
    "\\newcommand{\\termu}{{\\mathrm{term}_U}}\n",
    "\\newcommand{\\terms}{{\\mathrm{term}_S}}\n",
    "\\newcommand{\\termv}{{\\mathrm{term}_V}}\n",
    "$$\n",
    "$ \\tr(\\gAt \\dA) = \\tr(\\gUt \\dU) + \\tr(\\gSt \\dS) + \\tr(\\gVt \\dV) $\n",
    "\n",
    "##### Gradient formula\n",
    "\n",
    "The formula for A's gradient has terms $\\termu$, $\\terms$, and $\\termv$ found from the respective trace terms in the chain rule\n",
    "\n",
    "$\\gA = \\termu + \\terms + \\termv$\n",
    "\n",
    "$\\termu = [\\U (\\F \\circ [\\Ut \\gU - \\gUt \\U]) \\S + (\\I_m - \\U \\Ut) \\gU \\S^{-1} ] \\Vt$\n",
    "\n",
    "$\\terms = \\U (\\I_k \\circ \\gS ) \\Vt$\n",
    "\n",
    "$\\termv = \\U [\\S (\\F \\circ [\\Vt \\gV - \\gVt \\V]) \\Vt + \\S^{-1} \\gVt (\\I_n - \\V \\Vt)]$\n",
    "\n",
    "##### Numerical instability\n",
    "$\\S^{-1}$ and $\\F$ cause numerical instability in $\\gA$ through $\\termu$ and $\\termv$. The explanation is the same as in the forward mode.\n",
    "\n",
    "##### Computing only singular values\n",
    "When only computing singular values, no $\\U$ or $\\Vt$ were produced to impact a resulting objective function. $\\gU$ and $\\gVt$ must then be zero, and so are $\\termu$ and $\\termv$. This means that $\\gA = \\terms$, and there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32dbfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_partial(A, U, S_vals, Vt, gU, gS_vals, gVt):\n",
    "    S = jnp.diag(S_vals)\n",
    "    gS = jnp.diag(gS_vals)\n",
    "    \n",
    "    k = S.shape[0]\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    I_m = jnp.eye(m)\n",
    "    I_k = jnp.eye(k)\n",
    "    I_n = jnp.eye(n)\n",
    "    \n",
    "    V = Vt.T\n",
    "    Ut = U.T\n",
    "    gUt = gU.T\n",
    "    gV = gVt.T\n",
    "    \n",
    "    S_inv = jnp.linalg.inv(S)\n",
    "    \n",
    "    F_ = F(S_vals, k)\n",
    "    \n",
    "    term_U = (U @ (F_ * (Ut @ gU - gUt @ U)) @ S + (I_m - U @ Ut) @ gU @ S_inv) @ Vt\n",
    "    \n",
    "    term_S = U @ (I_k * gS) @ Vt\n",
    "    \n",
    "    term_V = U @ (S @ (F_ * (Vt @ gV - gVt @ V)) @ Vt + S_inv @ gVt @ (I_n - V @ Vt))\n",
    "    \n",
    "    gA = term_U + term_S + term_V\n",
    "    \n",
    "    return gA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08d7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_partial_(A, gU, gS, gVt):\n",
    "    A = jnp.array(A)\n",
    "    gU = jnp.array(gU)\n",
    "    gS = jnp.array(gS)\n",
    "    gVt = jnp.array(gVt)\n",
    "    \n",
    "    U, S, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=False)\n",
    "    \n",
    "    gA = svd_vjp_real_valued_partial(A, U, S, Vt, gU, gS, gVt)\n",
    "    \n",
    "    return gA\n",
    "\n",
    "def jax_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    A = jnp.array(A)\n",
    "    gU = jnp.array(gU)\n",
    "    gS = jnp.array(gS)\n",
    "    gVt = jnp.array(gVt)    \n",
    "    \n",
    "    _, vjp_fun = jax.vjp(\n",
    "        lambda A: jnp.linalg.svd(A, compute_uv=True, full_matrices=False), \n",
    "        A,\n",
    "    )\n",
    "    \n",
    "    gA, = vjp_fun((gU, gS, gVt))\n",
    "    \n",
    "    return gA\n",
    "\n",
    "def pytorch_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    A = torch.tensor(A)\n",
    "    gU = torch.tensor(gU)\n",
    "    gS = torch.tensor(gS)\n",
    "    gVt = torch.tensor(gVt)\n",
    "    \n",
    "    _, gA = torch_vjp(lambda A: torch.linalg.svd(A, full_matrices=False), A, (gU, gS, gVt))\n",
    "    \n",
    "    return gA\n",
    "    \n",
    "def check_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    jax_gA = jax_real_valued_partial_vjp(A, gU, gS, gVt)\n",
    "    torch_gA = pytorch_real_valued_partial_vjp(A, gU, gS, gVt)\n",
    "    res_gA = svd_vjp_real_valued_partial_(A, gU, gS, gVt)\n",
    "    \n",
    "    # example == jax == torch\n",
    "    assert_allclose(jax_gA, torch_gA)\n",
    "    assert_allclose(jax_gA, res_gA)\n",
    "    \n",
    "    return jax_gA, torch_gA, res_gA\n",
    "    \n",
    "def check_real_valued_partial_vjp_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "\n",
    "    k = min(m, n)\n",
    "\n",
    "    gU = jnp.ones((m, k))\n",
    "    gS = jnp.ones(k)\n",
    "    gVt = jnp.ones((k, n))\n",
    "\n",
    "    A = A.tolist()\n",
    "    gU = gU.tolist()\n",
    "    gS = gS.tolist()\n",
    "    gVt = gVt.tolist()\n",
    "    \n",
    "    return A, gU, gS, gVt\n",
    "\n",
    "def check_real_valued_partials_vjp():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_partial_vjp_args(subkey, m, n)\n",
    "            check_real_valued_partial_vjp(*args)\n",
    "\n",
    "            \n",
    "check_real_valued_partials_vjp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b24fa",
   "metadata": {},
   "source": [
    "## Real valued full SVD\n",
    "\n",
    "Reference: https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "\n",
    "### Forward mode\n",
    "\n",
    "The differential formulas are found by a similar process to the partial case.\n",
    "\n",
    "##### Differential formulas\n",
    "\n",
    "$ \\dU = \\U [\\F \\circ ( \\dP_1 \\S_1 + \\S_1 \\dP_1^{\\top} ) ] $\n",
    "\n",
    "$ \\Ut \\dA \\V = \\begin{bmatrix} \\dP_1 & \\dP_2 \\end{bmatrix} $\n",
    "\n",
    "$ \\S = \\begin{bmatrix} \\S_1 & 0 \\end{bmatrix} $\n",
    "\n",
    "$ \\dP_1 $ is $m \\times m$\n",
    "\n",
    "$ \\S_1 $ is $m \\times m$\n",
    "\n",
    "---\n",
    "\n",
    "$ \\dS = \\I \\circ [\\U^{\\top} \\dA \\V ] $\n",
    "\n",
    "---\n",
    "\n",
    "$ \\dV = \\V \\dD $\n",
    "\n",
    "$ \\dD = \\begin{bmatrix} \\dD_1 & -\\dD_2 \\\\ \\dD_2^{\\top} & \\dD_3 \\end{bmatrix} $\n",
    "\n",
    "$ \\dD_1 = \\F \\circ (\\S_1 \\dP_1 + \\dP_1^{\\top} \\S_1 ) $\n",
    "\n",
    "$ \\dD_2 = \\S_1^{-1} \\dP_2 $\n",
    "\n",
    "$ \\dD_3 = \\mathbf{0} $\n",
    "\n",
    "$ \\dD $ is $ n \\times n $\n",
    "\n",
    "$ \\dD_1 $ is $ m \\times m $\n",
    "\n",
    "$ \\dD_2 $ is $ m \\times (n - m) $\n",
    "\n",
    "$ \\dD_3 $ is $ (n - m) \\times (n - m) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91523b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_full(A, dA):\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    if m > n:\n",
    "        (U, S, Vt), (dU, dS, dVt) = svd_jvp_real_valued_full(A.T, dA.T)\n",
    "        return (Vt.T, S, U.T), (dVt.T, dS, dU.T)\n",
    "    \n",
    "    # m <= n\n",
    "    \n",
    "    U, S_vals, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=True)\n",
    "        \n",
    "    Ut = U.T\n",
    "    V = Vt.T\n",
    "    \n",
    "    I = fill_diagonal(jnp.zeros((m, n)), 1)\n",
    "\n",
    "    # Can directly create S_1 without creating S which has added zero columns\n",
    "    # There will be m singular values, so S_1 will be m x m.\n",
    "    S_1 = jnp.diag(S_vals)\n",
    "    S_1_inv = jnp.linalg.inv(S_1)\n",
    "    \n",
    "    # m is number singular values insted of k as in partial version\n",
    "    F_ = F(S_vals, m)\n",
    "    \n",
    "    ########## dU ###########\n",
    "\n",
    "    dP = Ut @ dA @ V\n",
    "    dP_1 = dP[:, :m]\n",
    "    dP_2 = dP[:, m:]\n",
    "    dP_1t = dP_1.T\n",
    "    \n",
    "    dU = U @ (F_ * (dP_1 @ S_1 + S_1 @ dP_1t))\n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ######### dS ##########\n",
    "    \n",
    "    dS = I * (Ut @ dA @ V)\n",
    "    dS_vals = jnp.diagonal(dS)\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    ######## dV ##########\n",
    "    \n",
    "    dD_1 = F_ * (S_1 @ dP_1 + dP_1t @ S_1)\n",
    "    dD_2 = S_1_inv @ dP_2\n",
    "    dD_3 = jnp.zeros((n-m, n-m))\n",
    "    neg_dD_2 = -dD_2\n",
    "    dD_2t = dD_2.T\n",
    "    \n",
    "    dD_top = jnp.concatenate((dD_1, neg_dD_2), axis=1)\n",
    "    dD_bottom = jnp.concatenate((dD_2t, dD_3), axis=1)\n",
    "    \n",
    "    dD = jnp.concatenate((dD_top, dD_bottom), axis=0)\n",
    "    \n",
    "    dV = V @ dD\n",
    "    \n",
    "    #####################\n",
    "    \n",
    "    return (U, S_vals, Vt), (dU, dS_vals, dV.T)\n",
    "\n",
    "# modified version of fill_diagonal because jax does not implement it.\n",
    "# https://github.com/numpy/numpy/blob/v1.23.0/numpy/lib/index_tricks.py#L779-L910\n",
    "def fill_diagonal(a, val, start=None):\n",
    "    m, n = a.shape\n",
    "    \n",
    "    end = None\n",
    "    step = a.shape[1] + 1\n",
    "    \n",
    "    if start:\n",
    "        start_row, start_col = start\n",
    "        start = start_row * n + start_col\n",
    "        a = a.flatten().at[start:end:step].set(val)\n",
    "    else:\n",
    "        a = a.flatten().at[:end:step].set(val)\n",
    "        \n",
    "    a = a.reshape((m, n))\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df01bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_full_(A, dA):\n",
    "    A = jnp.array(A)\n",
    "    dA = jnp.array(dA)\n",
    "    \n",
    "    return svd_jvp_real_valued_full(A, dA)\n",
    "    \n",
    "def pytorch_real_valued_full(A, dA):\n",
    "    A = torch.tensor(A)\n",
    "    dA = torch.tensor(dA)\n",
    "    \n",
    "    return torch_jvp(lambda A: torch.linalg.svd(A, full_matrices=True), A, dA)\n",
    "\n",
    "def check_real_valued_full(A, dA):\n",
    "    # NOTE - JAX does not implement full\n",
    "    torch_res = pytorch_real_valued_partial(A, dA)\n",
    "    res = svd_jvp_real_valued_partial(jnp.array(A), jnp.array(dA))\n",
    "    \n",
    "    assert_svd_uv(torch_res, res)\n",
    "    \n",
    "    return torch_res, res\n",
    "\n",
    "def check_real_valued_full_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "    dA = jnp.ones_like(A)\n",
    "            \n",
    "    A = A.tolist()\n",
    "    dA = dA.tolist()\n",
    "    \n",
    "    return A, dA\n",
    "  \n",
    "def check_real_valued_fulls():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_full_args(subkey, m, n)\n",
    "            check_real_valued_full(*args)\n",
    "\n",
    "check_real_valued_fulls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be40654",
   "metadata": {},
   "source": [
    "### Backward mode\n",
    "\n",
    "The backward mode for the full case is found by the same process as in the partial case, but the complete formula is not documented in the reference. \n",
    "\n",
    "##### Gradient formula\n",
    "\n",
    "$\\gA = \\termu + \\terms + \\termv$\n",
    "\n",
    "$\\termu = \\V \\I_{P1} ( \\S_1 [\\F \\circ (\\gUt \\U)] + [\\F \\circ (\\gUt \\U)] \\S_1  ) \\Ut $\n",
    "\n",
    "$\\terms = \\U (\\I \\circ \\gSt) \\Vt$\n",
    "\n",
    "$\\termv = \\V \\I_{P1} ( \\S_1 [\\F \\circ ( \\I_{D1R} \\gVt \\V \\I_{D1L} )] + [\\F \\circ ( \\I_{D1R} \\gVt \\V \\I_{D1R} ) ] \\S_1 ) \\Ut + \\V \\I_{P2} [ \\I_{D2L}^{\\top} \\Vt \\gV \\I_{D2R}^{\\top} \\S_1^{-1} + \\I_{D3R} \\gVt \\gV \\I_{D3L} (-S_1^{-1}) ] \\Ut $\n",
    "\n",
    "The $\\I_{P}$ terms are used to select $\\dP_1$ and $\\dP_2$ out of $\\dP$\n",
    "\n",
    "$ \\dP_1 = \\dP \\I_{P1} $\n",
    "\n",
    "$ \\dP_2 = \\dP \\I_{P2} $\n",
    "\n",
    "The $\\I_{D}$ terms are used to augment the $\\dD_1$, $\\dD_2^{\\top}$, and $-\\dD_2$ terms so $\\dD$ can be represented as a sum. $\\dD_3$ is the zero matrix, so it can be ignored.\n",
    "\n",
    "$ \\dD = \\I_{D1L} \\dD_1 \\I_{D1R} + \\I_{D2L} \\dD_2^{\\top} \\I_{D2R} + \\I_{D3L} (-\\dD_2) \\I_{D3R} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1610d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_full(A, U, S_vals, Vt, gU, gS_vals, gVt):\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    if m > n:\n",
    "        gA = svd_vjp_real_valued_full(A.T, Vt.T, S_vals, U.T, gVt.T, gS_vals, gU.T)\n",
    "        return gA.T\n",
    "    \n",
    "    # m <= n\n",
    "    \n",
    "    S = fill_diagonal(jnp.zeros((m, n)), S_vals)\n",
    "    gS = fill_diagonal(jnp.zeros((m, n)), gS_vals)\n",
    "    S_1 = fill_diagonal(jnp.zeros((m, m)), S_vals)\n",
    "    S_1t = S_1.T\n",
    "    S_1_inv = jnp.linalg.inv(S_1)\n",
    "    \n",
    "    F_ = F(S_vals, m)\n",
    "    Ft = F_.T\n",
    "    \n",
    "    V = Vt.T\n",
    "    Ut = U.T\n",
    "    gSt = gS.T\n",
    "    gV = gVt.T\n",
    "    \n",
    "    # Vm is the first m cols of V\n",
    "    Vm = V[:,:m]\n",
    "    # Vr is the last n-m cols of V\n",
    "    Vr = V[:,m:]\n",
    "    gVm = gV[:,:m]\n",
    "    gVr = gV[:,m:]\n",
    "    Vmt = Vm.T\n",
    "    Vrt = Vr.T\n",
    "    gVmt = gVm.T\n",
    "    gVrt = gVr.T\n",
    "    \n",
    "    ############ term_U ##########\n",
    "    \n",
    "    J_u = F_ * (Ut @ gU)\n",
    "    J_ut = J_u.T\n",
    "    term_U = U @ (J_u + J_ut) @ S_1 @ Vmt\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ########## term_S ###########\n",
    "    \n",
    "    I = fill_diagonal(jnp.zeros((m, n)), 1)\n",
    "    term_S = U @ (I * gS) @ Vt\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ######### term_V ############\n",
    "    \n",
    "    I_D1L, I_D1R = I_D1(m, n)\n",
    "    I_D2L, I_D2R = I_D2(m, n)\n",
    "    I_D3L, I_D3R = I_D3(m, n)\n",
    "    \n",
    "    I_D3Lt = I_D3L.T\n",
    "    I_D3Rt = I_D3R.T\n",
    "\n",
    "    J_v = Ft * (I_D1R @ gVt @ V @ I_D1L)\n",
    "    J_vt = J_v.T\n",
    "    \n",
    "    term_V1 = U @ S_1t @ (J_v + J_vt) @ Vmt\n",
    "    term_V2 = U @ S_1_inv @ I_D2R @ gVt @ V @ I_D2L @ Vrt\n",
    "    term_V3 = -U @ S_1_inv @ I_D3Lt @ Vt @ gV @ I_D3Rt @ Vrt\n",
    "    \n",
    "    term_V = term_V1 + term_V2 + term_V3\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    gA = term_U + term_S + term_V\n",
    "    \n",
    "    return gA\n",
    "\n",
    "# Expand top left m x m to n x n\n",
    "# n x m @ m x m @ m x n\n",
    "def I_D1(m, n):\n",
    "    L = jnp.zeros((n, m))\n",
    "    R = jnp.zeros((m, n))\n",
    "    \n",
    "    L = fill_diagonal(L, 1.)\n",
    "    R = fill_diagonal(R, 1.)\n",
    "    \n",
    "    return L, R\n",
    "\n",
    "# Expand botton left (n - m) x m to n x n\n",
    "# n x (n - m) @ (n - m) x m @ m x n\n",
    "def I_D2(m, n):\n",
    "    L = jnp.zeros((n, n - m))\n",
    "    R = jnp.zeros((m, n))\n",
    "    \n",
    "    L = fill_diagonal(L, 1., (m, 0))\n",
    "    R = fill_diagonal(R, 1.)\n",
    "    \n",
    "    return L, R\n",
    "\n",
    "# Expand top right m x (n - m) to n x n\n",
    "# n x m @ m x (n - m) @ (n - m) x n\n",
    "def I_D3(m, n):\n",
    "    L = jnp.zeros((n, m))\n",
    "    R = jnp.zeros((n-m, n))\n",
    "    \n",
    "    L = fill_diagonal(L, 1.)\n",
    "    R = fill_diagonal(R, 1., (0, m))    \n",
    "    \n",
    "    return L, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7cea8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_full_(A, gU, gS, gVt):\n",
    "    A = jnp.array(A)\n",
    "    gU = jnp.array(gU)\n",
    "    gS = jnp.array(gS)\n",
    "    gVt = jnp.array(gVt)\n",
    "    \n",
    "    U, S, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=True)\n",
    "    \n",
    "    gA = svd_vjp_real_valued_full(A, U, S, Vt, gU, gS, gVt)\n",
    "    \n",
    "    return gA\n",
    "    \n",
    "def tf_real_valued_full_vjp(A, gU, gS, gVt):\n",
    "    A = tf.constant(A)\n",
    "    gU = tf.constant(gU)\n",
    "    gS = tf.constant(gS)\n",
    "    gVt = tf.constant(gVt)\n",
    "    \n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(A)\n",
    "        S, U, V = tf.linalg.svd(A, full_matrices=True, compute_uv=True)\n",
    "        Vt = tf.transpose(V)\n",
    "        \n",
    "    gA = g.gradient((S, U, Vt), A, (gS, gU, gVt))\n",
    "        \n",
    "    return gA\n",
    "\n",
    "def check_real_valued_full_vjp(A, gU, gS, gVt):\n",
    "    tf_gA = tf_real_valued_full_vjp(A, gU, gS, gVt)\n",
    "    res_gA = svd_vjp_real_valued_full_(A, gU, gS, gVt)\n",
    "    \n",
    "    assert_allclose(tf_gA, res_gA)\n",
    "    \n",
    "    return tf_gA, res_gA\n",
    "\n",
    "def check_real_valued_full_vjp_args(key, m, n):\n",
    "    while True:\n",
    "        key, subkey = random.split(key)\n",
    "        A = random.normal(key, (m, n))\n",
    "        \n",
    "        tf_S, tf_U, tf_V = tf.linalg.svd(tf.constant(A.tolist()), full_matrices=True, compute_uv=True)\n",
    "        tf_Vt = tf.transpose(tf_V)\n",
    "        \n",
    "        jax_U, jax_S, jax_Vt = jnp.linalg.svd(A, full_matrices=True, compute_uv=True)\n",
    "        \n",
    "        if check_allclose(jax_U, tf_U) and check_allclose(jax_S, tf_S) and check_allclose(jax_Vt, tf_Vt):\n",
    "            break\n",
    "\n",
    "    k = min(m, n)\n",
    "\n",
    "    gU = jnp.zeros((m, m))\n",
    "    gS = jnp.zeros(k)\n",
    "    gVt = jnp.ones((n, n))\n",
    "\n",
    "    A = A.tolist()\n",
    "    gU = gU.tolist()\n",
    "    gS = gS.tolist()\n",
    "    gVt = gVt.tolist()\n",
    "\n",
    "    return key, (A, gU, gS, gVt)\n",
    "  \n",
    "def check_real_valued_fulls_vjp():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(2, 11):\n",
    "        min_n = max(m - 1, 2)\n",
    "        max_n = min(m + 2, 11)\n",
    "        for n in range(min_n, max_n):\n",
    "            assert(abs(m - n) < 2)\n",
    "            key, args = check_real_valued_full_vjp_args(key, m, n)\n",
    "            check_real_valued_full_vjp(*args)\n",
    "\n",
    "check_real_valued_fulls_vjp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72312621",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "SVD real differentiability:\n",
    "- https://j-towns.github.io/papers/svd-derivative.pdf\n",
    "- https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "- https://arxiv.org/pdf/1509.07838.pdf\n",
    "\n",
    "SVD complex differentiability:\n",
    "- https://arxiv.org/pdf/1909.02659.pdf\n",
    "- https://giggleliu.github.io/2019/04/02/einsumbp.html\n",
    "\n",
    "Existing implementations:\n",
    "\n",
    "Jax forward:\n",
    "- https://github.com/google/jax/blob/2a00533e3e686c1c9d7dfe9ed2a3b19217cfe76f/jax/_src/lax/linalg.py#L1578\n",
    "- Jax only implements the forward rule because jax can derive the backward rule from the forward rule and vice versa.\n",
    "\n",
    "Pytorch forward:\n",
    "- https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3122\n",
    "\n",
    "Pytorch backward:\n",
    "- https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3228\n",
    "\n",
    "Tensorflow backward:\n",
    "- https://github.com/tensorflow/tensorflow/blob/bbe41abdcb2f7e923489bfa21cfb546b6022f330/tensorflow/python/ops/linalg_grad.py#L815\n",
    "\n",
    "General complex differentiability:\n",
    "- https://mediatum.ub.tum.de/doc/631019/631019.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
