{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b819698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import jax\n",
    "from jax import random\n",
    "\n",
    "import torch\n",
    "import torch.autograd.functional as TF\n",
    "torch_jvp = TF.jvp\n",
    "torch_vjp = TF.vjp\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05bc7a9",
   "metadata": {},
   "source": [
    "## Cases\n",
    "$$\n",
    "\\newcommand{\\tr}{{\\mathrm{tr}}}\n",
    "\\newcommand{\\d}{{\\mathrm{d}}}\n",
    "\\newcommand{\\A}{{\\mathbf{A}}}\n",
    "\\newcommand{\\U}{{\\mathbf{U}}}\n",
    "\\newcommand{\\S}{{\\mathbf{S}}}\n",
    "\\newcommand{\\V}{{\\mathbf{V}}}\n",
    "\\newcommand{\\F}{{\\mathbf{F}}}\n",
    "\\newcommand{\\I}{{\\mathbf{I}}}\n",
    "\\newcommand{\\P}{{\\mathbf{P}}}\n",
    "\\newcommand{\\D}{{\\mathbf{D}}}\n",
    "\\newcommand{\\dA}{{\\d\\A}}\n",
    "\\newcommand{\\dU}{{\\d\\U}}\n",
    "\\newcommand{\\dS}{{\\d\\S}}\n",
    "\\newcommand{\\dV}{{\\d\\V}}\n",
    "\\newcommand{\\dP}{{\\d\\P}}\n",
    "\\newcommand{\\dD}{{\\d\\D}}\n",
    "\\newcommand{\\Ut}{{\\U^{\\top}}}\n",
    "\\newcommand{\\Vt}{{\\V^{\\top}}}\n",
    "\\newcommand{\\Vh}{{\\V^{H}}}\n",
    "\\newcommand{\\dAt}{{\\dA^{\\top}}}\n",
    "\\newcommand{\\dVt}{{\\dV^{\\top}}}\n",
    "\\newcommand{\\gA}{{\\overline{\\A}}}\n",
    "\\newcommand{\\gAt}{{\\gA^{\\top}}}\n",
    "\\newcommand{\\gU}{{\\overline{\\U}}}\n",
    "\\newcommand{\\gUt}{{\\gU^{\\top}}}\n",
    "\\newcommand{\\gS}{{\\overline{\\S}}}\n",
    "\\newcommand{\\gSt}{{\\gS^{\\top}}}\n",
    "\\newcommand{\\gV}{{\\overline{\\V}}}\n",
    "\\newcommand{\\gVt}{{\\gV^{\\top}}}\n",
    "$$\n",
    "The derivative of the SVD operation is determined by\n",
    "\n",
    "1. Computing the full SVD vs the \"thin\"/\"partial\" SVD\n",
    "2. complex vs real inputs\n",
    "3. Computing the complete factorization $\\U\\S\\Vh$ vs computing only the singular values $\\S$\n",
    "\n",
    "Computing only the singular values does not change the differential or gradient formulas and can be considered a side case of the full factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac90ffc",
   "metadata": {},
   "source": [
    "## Real valued partial SVD\n",
    "\n",
    "Reference: https://j-towns.github.io/papers/svd-derivative.pdf\n",
    "\n",
    "### Forward mode\n",
    "\n",
    "The differential formulas $\\dU$, $\\dS$, and $\\dV$ in terms of $\\dA$, $\\U$, $\\S$, and $\\V$ are found from the chain rule.\n",
    "\n",
    "##### Chain rule\n",
    "\n",
    "$\\dA = \\dU \\S \\Vt + \\U \\dS \\Vt + \\U \\S \\dVt$\n",
    "\n",
    "TODO(will) - Is this really the chain rule? Deriving this is a little funky because you end up taking partial derivatives with respect to matrices which I'm not sure how to define. The rule for expressing the differential and the product is in 2.1/2.2 https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "\n",
    "##### Differential formulas\n",
    "\n",
    "$\\dU = \\U ( \\F \\circ [\\Ut \\dA \\V \\S + \\S \\Vt \\dAt \\U] ) + (\\I_m - \\U \\Ut ) \\dA \\V \\S^{-1}$\n",
    "\n",
    "$\\dS = \\I_k \\circ [\\Ut \\dA \\V]$\n",
    "\n",
    "$\\dV = \\V (\\F \\circ [\\S \\Ut \\dA \\V + \\Vt \\dAt \\U \\S]) + (\\I_n - \\V \\Vt) \\dAt \\U \\S^{-1}$\n",
    "\n",
    "where\n",
    "\n",
    "$F_{ij} = \\frac{1}{s_j^2 - s_i^2}, i \\neq j$\n",
    "\n",
    "$F_{ij} = 0$ otherwise\n",
    "\n",
    "$k$ is the rank of $\\A$\n",
    "\n",
    "$\\U$ is $m \\times k$\n",
    "\n",
    "$\\S$ is $k \\times k$\n",
    "\n",
    "$\\Vt$ is $k \\times n$\n",
    "\n",
    "##### Numerical instability\n",
    "Both $\\S^{-1}$ and $\\F$ cause numerical instability in $\\dU$ and $\\dV$.  \n",
    "\n",
    "$\\S^{-1}$ causes numerical instability with zero or very small singular values. The inverse of a diagonal matrix is the element wise reciprocals of the inverting matrix's diagonal. Zero singular values will cause $\\S^{-1}$ to have infinite elements. Very small singular values will cause $\\S^{-1}$ to have very large floating point elements.\n",
    "\n",
    "$\\F$ divides by the difference of all pairs of singular values. Repeated singular values will cause infinite elements of $\\F$. Close singular values will cause very large floating point elements of $\\F$.  \n",
    "\n",
    "##### Computing only singular values\n",
    "Only compute the differential for $\\S$. $\\dU$ and $\\dV$ are ignored. Because $\\dU$ and $\\dV$ are not computed, $\\F$ and $\\S^{-1}$ are not needed, so there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56584cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_partial(A, dA):    \n",
    "    U, S_vals, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=False)\n",
    "\n",
    "    S = jnp.diag(S_vals)\n",
    "    Ut = U.T\n",
    "    V = Vt.T\n",
    "    dAt = dA.T\n",
    "\n",
    "    k = S.shape[0]\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "\n",
    "    I_k = jnp.eye(k)\n",
    "    I_m = jnp.eye(m)\n",
    "    I_n = jnp.eye(n)\n",
    "\n",
    "    S_inv = jnp.linalg.inv(S)\n",
    "\n",
    "    F_ = F(S_vals, k)\n",
    "\n",
    "    dU = U @ (F_ * ((Ut @ dA @ V @ S) + (S @ Vt @ dAt @ U))) + ((I_m - U @ Ut) @ dA @ V @ S_inv)\n",
    "    \n",
    "    # Note that the `I_k *` is extraneous. It zeros out the rest of the matrix besides the diagonal.\n",
    "    # We only return `dS_vals` which takes only the diagonal of `dS` anyway. \n",
    "    dS = I_k * (Ut @ dA @ V)\n",
    "    dS_vals = jnp.diagonal(dS)\n",
    "    \n",
    "    dV = V @ (F_ * (S @ Ut @ dA @ V + Vt @ dAt @ U @ S)) + (I_n - V @ Vt) @ dAt @ U @ S_inv\n",
    "    \n",
    "    return (U, S_vals, Vt), (dU, dS_vals, dV.T)\n",
    "\n",
    "def F(S_vals, k):\n",
    "    F_i_j = lambda i, j: lax.cond(i == j, lambda: 0., lambda: 1 / (S_vals[j]**2 - S_vals[i]**2))\n",
    "    F_fun = jax.vmap(jax.vmap(F_i_j, (None, 0)), (0, None))\n",
    "\n",
    "    indices = jnp.arange(k)\n",
    "    F_ = F_fun(indices, indices)\n",
    "    \n",
    "    return F_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b00e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_svd_uv(a, b):\n",
    "    (U, S, Vt), (dU, dS, dVt) = a\n",
    "    (U_, S_, Vt_), (dU_, dS_, dVt_) = b\n",
    "    \n",
    "    assert_allclose(U, U_)\n",
    "    assert_allclose(S, S_)\n",
    "    assert_allclose(Vt, Vt_)\n",
    "    assert_allclose(dU, dU_)\n",
    "    assert_allclose(dS, dS_)\n",
    "    assert_allclose(dVt, dVt_)\n",
    "\n",
    "def assert_allclose(l, r):\n",
    "    assert(jnp.allclose(jnp.array(l), jnp.array(r), atol=1e-04))\n",
    "        \n",
    "def jax_real_valued_partial(A, dA):\n",
    "    A = jnp.array(A)\n",
    "    dA = jnp.array(dA)\n",
    "    \n",
    "    return jax.jvp(\n",
    "        lambda A: jnp.linalg.svd(A, compute_uv=True, full_matrices=False), \n",
    "        (A,), \n",
    "        (dA,)\n",
    "    )\n",
    "\n",
    "def pytorch_real_valued_partial(A, dA):\n",
    "    A = torch.tensor(A)\n",
    "    dA = torch.tensor(dA)\n",
    "    \n",
    "    return torch_jvp(lambda A: torch.linalg.svd(A, full_matrices=False), A, dA)\n",
    "    \n",
    "def check_real_valued_partial(A, dA):\n",
    "    jax_res = jax_real_valued_partial(A, dA)\n",
    "    torch_res = pytorch_real_valued_partial(A, dA)\n",
    "    res = svd_jvp_real_valued_partial(jnp.array(A), jnp.array(dA))\n",
    "    \n",
    "    # example == jax == torch\n",
    "    assert_svd_uv(jax_res, torch_res)\n",
    "    assert_svd_uv(jax_res, res)\n",
    "    \n",
    "    return jax_res, torch_res, res\n",
    "\n",
    "def check_real_valued_partial_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "    dA = jnp.ones_like(A)\n",
    "            \n",
    "    A = A.tolist()\n",
    "    dA = dA.tolist()\n",
    "    \n",
    "    return A, dA\n",
    "  \n",
    "def check_real_valued_partials():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_partial_args(subkey, m, n)\n",
    "            check_real_valued_partial(*args)\n",
    "\n",
    "check_real_valued_partials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc7a36",
   "metadata": {},
   "source": [
    "### Reverse mode\n",
    "\n",
    "##### Chain rule\n",
    "$$\n",
    "\\newcommand{\\termu}{{\\mathrm{term}_U}}\n",
    "\\newcommand{\\terms}{{\\mathrm{term}_S}}\n",
    "\\newcommand{\\termv}{{\\mathrm{term}_V}}\n",
    "$$\n",
    "$ \\tr(\\gAt \\dA) = \\tr(\\gUt \\dU) + \\tr(\\gSt \\dS) + \\tr(\\gVt \\dV) $\n",
    "\n",
    "##### Gradient formula\n",
    "\n",
    "The formula for A's gradient has terms $\\termu$, $\\terms$, and $\\termv$ found from the respective trace terms in the chain rule\n",
    "\n",
    "$\\gA = \\termu + \\terms + \\termv$\n",
    "\n",
    "$\\termu = [\\U (\\F \\circ [\\Ut \\gU - \\gUt \\U]) \\S + (\\I_m - \\U \\Ut) \\gU \\S^{-1} ] \\Vt$\n",
    "\n",
    "$\\terms = \\U (\\I_k \\circ \\gS ) \\Vt$\n",
    "\n",
    "$\\termv = \\U [\\S (\\F \\circ [\\Vt \\gV - \\gVt \\V]) \\Vt + \\S^{-1} \\gVt (\\I_n - \\V \\Vt)]$\n",
    "\n",
    "##### Numerical instability\n",
    "$\\S^{-1}$ and $\\F$ cause numerical instability in $\\gA$ through $\\termu$ and $\\termv$. The explanation is the same as in the forward mode.\n",
    "\n",
    "##### Computing only singular values\n",
    "When only computing singular values, no $\\U$ or $\\Vt$ were produced to impact a resulting objective function. $\\gU$ and $\\gVt$ must then be zero, and so are $\\termu$ and $\\termv$. This means that $\\gA = \\terms$, and there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32dbfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_partial(A, U, S_vals, Vt, gU, gS_vals, gVt):\n",
    "    S = jnp.diag(S_vals)\n",
    "    gS = jnp.diag(gS_vals)\n",
    "    \n",
    "    k = S.shape[0]\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    I_m = jnp.eye(m)\n",
    "    I_k = jnp.eye(k)\n",
    "    I_n = jnp.eye(n)\n",
    "    \n",
    "    V = Vt.T\n",
    "    Ut = U.T\n",
    "    gUt = gU.T\n",
    "    gV = gVt.T\n",
    "    \n",
    "    S_inv = jnp.linalg.inv(S)\n",
    "    \n",
    "    F_ = F(S_vals, k)\n",
    "    \n",
    "    term_U = (U @ (F_ * (Ut @ gU - gUt @ U)) @ S + (I_m - U @ Ut) @ gU @ S_inv) @ Vt\n",
    "    \n",
    "    term_S = U @ (I_k * gS) @ Vt\n",
    "    \n",
    "    term_V = U @ (S @ (F_ * (Vt @ gV - gVt @ V)) @ Vt + S_inv @ gVt @ (I_n - V @ Vt))\n",
    "    \n",
    "    gA = term_U + term_S + term_V\n",
    "    \n",
    "    return gA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "befd90e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_partial_(A, gU, gS, gVt):\n",
    "    A = jnp.array(A)\n",
    "    gU = jnp.array(gU)\n",
    "    gS = jnp.array(gS)\n",
    "    gVt = jnp.array(gVt)\n",
    "    \n",
    "    U, S, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=False)\n",
    "    \n",
    "    gA = svd_vjp_real_valued_partial(A, U, S, Vt, gU, gS, gVt)\n",
    "    \n",
    "    return gA\n",
    "\n",
    "def jax_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    A = jnp.array(A)\n",
    "    gU = jnp.array(gU)\n",
    "    gS = jnp.array(gS)\n",
    "    gVt = jnp.array(gVt)    \n",
    "    \n",
    "    _, vjp_fun = jax.vjp(\n",
    "        lambda A: jnp.linalg.svd(A, compute_uv=True, full_matrices=False), \n",
    "        A,\n",
    "    )\n",
    "    \n",
    "    gA, = vjp_fun((gU, gS, gVt))\n",
    "    \n",
    "    return gA\n",
    "\n",
    "def pytorch_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    A = torch.tensor(A)\n",
    "    gU = torch.tensor(gU)\n",
    "    gS = torch.tensor(gS)\n",
    "    gVt = torch.tensor(gVt)\n",
    "    \n",
    "    _, gA = torch_vjp(lambda A: torch.linalg.svd(A, full_matrices=False), A, (gU, gS, gVt))\n",
    "    \n",
    "    return gA\n",
    "    \n",
    "def check_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    jax_gA = jax_real_valued_partial_vjp(A, gU, gS, gVt)\n",
    "    torch_gA = pytorch_real_valued_partial_vjp(A, gU, gS, gVt)\n",
    "    res_gA = svd_vjp_real_valued_partial_(A, gU, gS, gVt)\n",
    "    \n",
    "    # example == jax == torch\n",
    "    assert_allclose(jax_gA, torch_gA)\n",
    "    assert_allclose(jax_gA, res_gA)\n",
    "    \n",
    "    return jax_gA, torch_gA, res_gA\n",
    "    \n",
    "def check_real_valued_partial_vjp_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "\n",
    "    k = min(m, n)\n",
    "\n",
    "    gU = jnp.ones((m, k))\n",
    "    gS = jnp.ones(k)\n",
    "    gVt = jnp.ones((k, n))\n",
    "\n",
    "    A = A.tolist()\n",
    "    gU = gU.tolist()\n",
    "    gS = gS.tolist()\n",
    "    gVt = gVt.tolist()\n",
    "    \n",
    "    return A, gU, gS, gVt\n",
    "\n",
    "def check_real_valued_partials_vjp():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_partial_vjp_args(subkey, m, n)\n",
    "            check_real_valued_partial_vjp(*args)\n",
    "\n",
    "            \n",
    "check_real_valued_partials_vjp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79435248",
   "metadata": {},
   "source": [
    "## Real valued full SVD\n",
    "\n",
    "Reference: https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "\n",
    "### Forward mode\n",
    "\n",
    "The differential formulas are found by a similar process to the partial case.\n",
    "\n",
    "##### Differential formulas\n",
    "\n",
    "$ \\dU = \\U [\\F \\circ ( \\dP_1 \\S_1 + \\S_1 \\dP_1^{\\top} ) ] $\n",
    "\n",
    "$ \\Ut \\dA \\V = \\begin{bmatrix} \\dP_1 & \\dP_2 \\end{bmatrix} $\n",
    "\n",
    "$ \\S = \\begin{bmatrix} \\S_1 & 0 \\end{bmatrix} $\n",
    "\n",
    "$ \\dP_1 $ is $m \\times m$\n",
    "\n",
    "$ \\S_1 $ is $m \\times m$\n",
    "\n",
    "---\n",
    "\n",
    "$ \\dS = \\I \\circ [\\U^{\\top} \\dA \\V ] $\n",
    "\n",
    "---\n",
    "\n",
    "$ \\dV = \\V \\dD $\n",
    "\n",
    "$ \\dD = \\begin{bmatrix} \\dD_1 & -\\dD_2 \\\\ \\dD_2^{\\top} & \\dD_3 \\end{bmatrix} $\n",
    "\n",
    "$ \\dD_1 = \\F \\circ (\\S_1 \\dP_1 + \\dP_1^{\\top} \\S_1 ) $\n",
    "\n",
    "$ \\dD_2 = \\S_1^{-1} \\dP_2 $\n",
    "\n",
    "$ \\dD_3 = \\mathbf{0} $\n",
    "\n",
    "$ \\dD $ is $ n \\times n $\n",
    "\n",
    "$ \\dD_1 $ is $ m \\times m $\n",
    "\n",
    "$ \\dD_2 $ is $ m \\times (n - m) $\n",
    "\n",
    "$ \\dD_3 $ is $ (n - m) \\times (n - m) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d77048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_full(A, dA):\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    if m > n:\n",
    "        (U, S, Vt), (dU, dS, dVt) = svd_jvp_real_valued_full(A.T, dA.T)\n",
    "        return (Vt.T, S, U.T), (dVt.T, dS, dU.T)\n",
    "    \n",
    "    # m <= n\n",
    "    \n",
    "    U, S_vals, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=True)\n",
    "        \n",
    "    Ut = U.T\n",
    "    V = Vt.T\n",
    "\n",
    "    # Can directly create S_1 without creating S which has added zero columns\n",
    "    # There will be m singular values, so S_1 will be m x m.\n",
    "    S_1 = jnp.diag(S_vals)\n",
    "    S_1_inv = jnp.linalg.inv(S_1)\n",
    "    \n",
    "    # m is number singular values insted of k as in partial version\n",
    "    F_ = F(S_vals, m)\n",
    "    \n",
    "    ########## dU ###########\n",
    "\n",
    "    dP = Ut @ dA @ V\n",
    "    dP_1 = dP[:, :m]\n",
    "    dP_2 = dP[:, m:]\n",
    "    dP_1t = dP_1.T\n",
    "    \n",
    "    dU = U @ (F_ * (dP_1 @ S_1 + S_1 @ dP_1t))\n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ######### dS ##########\n",
    "    \n",
    "    # Directly take the diagonal instead of taking an element\n",
    "    # wise product with an identity matrix\n",
    "    dS_vals = jnp.diagonal(Ut @ dA @ V)\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    ######## dV ##########\n",
    "    \n",
    "    dD_1 = F_ * (S_1 @ dP_1 + dP_1t @ S_1)\n",
    "    dD_2 = S_1_inv @ dP_2\n",
    "    dD_3 = jnp.zeros((n-m, n-m))\n",
    "    neg_dD_2 = -dD_2\n",
    "    dD_2t = dD_2.T\n",
    "    \n",
    "    dD_top = jnp.concatenate((dD_1, neg_dD_2), axis=1)\n",
    "    dD_bottom = jnp.concatenate((dD_2t, dD_3), axis=1)\n",
    "    \n",
    "    dD = jnp.concatenate((dD_top, dD_bottom), axis=0)\n",
    "    \n",
    "    dV = V @ dD\n",
    "    \n",
    "    #####################\n",
    "    \n",
    "    return (U, S_vals, Vt), (dU, dS_vals, dV.T)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374480b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_full_(A, dA):\n",
    "    A = jnp.array(A)\n",
    "    dA = jnp.array(dA)\n",
    "    \n",
    "    return svd_jvp_real_valued_full(A, dA)\n",
    "    \n",
    "def pytorch_real_valued_full(A, dA):\n",
    "    A = torch.tensor(A)\n",
    "    dA = torch.tensor(dA)\n",
    "    \n",
    "    return torch_jvp(lambda A: torch.linalg.svd(A, full_matrices=True), A, dA)\n",
    "\n",
    "def check_real_valued_full(A, dA):\n",
    "    # NOTE - JAX does not implment full\n",
    "    torch_res = pytorch_real_valued_partial(A, dA)\n",
    "    res = svd_jvp_real_valued_partial(jnp.array(A), jnp.array(dA))\n",
    "    \n",
    "    assert_svd_uv(torch_res, res)\n",
    "    \n",
    "    return torch_res, res\n",
    "\n",
    "def check_real_valued_full_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "    dA = jnp.ones_like(A)\n",
    "            \n",
    "    A = A.tolist()\n",
    "    dA = dA.tolist()\n",
    "    \n",
    "    return A, dA\n",
    "  \n",
    "def check_real_valued_fulls():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_full_args(subkey, m, n)\n",
    "            check_real_valued_full(*args)\n",
    "\n",
    "check_real_valued_fulls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72312621",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "SVD real differentiability:\n",
    "- https://j-towns.github.io/papers/svd-derivative.pdf\n",
    "- https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "- https://arxiv.org/pdf/1509.07838.pdf\n",
    "\n",
    "SVD complex differentiability:\n",
    "- https://arxiv.org/pdf/1909.02659.pdf\n",
    "- https://giggleliu.github.io/2019/04/02/einsumbp.html\n",
    "\n",
    "Existing implementations:\n",
    "\n",
    "Jax forward:\n",
    "- https://github.com/google/jax/blob/2a00533e3e686c1c9d7dfe9ed2a3b19217cfe76f/jax/_src/lax/linalg.py#L1578\n",
    "- Jax only implements the forward rule because jax can derive the backward rule from the forward rule and vice versa.\n",
    "\n",
    "Pytorch forward:\n",
    "- https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3122\n",
    "\n",
    "Pytorch backward:\n",
    "- https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3228\n",
    "\n",
    "Tensorflow forward:\n",
    "- https://github.com/tensorflow/tensorflow/blob/bbe41abdcb2f7e923489bfa21cfb546b6022f330/tensorflow/python/ops/linalg_grad.py#L815\n",
    "\n",
    "General complex differentiability:\n",
    "- https://mediatum.ub.tum.de/doc/631019/631019.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
