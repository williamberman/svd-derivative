{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b819698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import jax\n",
    "import torch\n",
    "import torch.autograd.functional as F\n",
    "torch_jvp = F.jvp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05bc7a9",
   "metadata": {},
   "source": [
    "## Cases\n",
    "$$\n",
    "\\newcommand{\\d}{{\\mathrm{d}}}\n",
    "\\newcommand{\\A}{{\\mathbf{A}}}\n",
    "\\newcommand{\\U}{{\\mathbf{U}}}\n",
    "\\newcommand{\\S}{{\\mathbf{S}}}\n",
    "\\newcommand{\\V}{{\\mathbf{V}}}\n",
    "\\newcommand{\\F}{{\\mathbf{F}}}\n",
    "\\newcommand{\\I}{{\\mathbf{I}}}\n",
    "\\newcommand{\\dA}{{\\d\\A}}\n",
    "\\newcommand{\\dU}{{\\d\\U}}\n",
    "\\newcommand{\\dS}{{\\d\\S}}\n",
    "\\newcommand{\\dV}{{\\d\\V}}\n",
    "\\newcommand{\\Ut}{{\\U^{\\top}}}\n",
    "\\newcommand{\\Vt}{{\\V^{\\top}}}\n",
    "\\newcommand{\\Vh}{{\\V^{H}}}\n",
    "\\newcommand{\\dAt}{{\\dA^{\\top}}}\n",
    "\\newcommand{\\dVt}{{\\dV^{\\top}}}\n",
    "\\newcommand{\\gA}{{\\overline{\\A}}}\n",
    "\\newcommand{\\gU}{{\\overline{\\U}}}\n",
    "\\newcommand{\\gUt}{{\\gU^{\\top}}}\n",
    "\\newcommand{\\gS}{{\\overline{\\S}}}\n",
    "\\newcommand{\\gSt}{{\\gS^{\\top}}}\n",
    "\\newcommand{\\gV}{{\\overline{\\V}}}\n",
    "\\newcommand{\\gVt}{{\\gV^{\\top}}}\n",
    "$$\n",
    "The derivative of the SVD operation is determined by\n",
    "\n",
    "1. Computing the full SVD vs the \"thin\"/\"partial\" SVD\n",
    "2. Computing the complete factorization $\\U\\S\\Vh$ vs computing just the singular values $\\S$\n",
    "3. complex vs real inputs\n",
    "\n",
    "These cases create 8 different cases for the SVD derivative each with separate differential formulas for forward mode AD update and adjoint formula for the reverse mode AD update.\n",
    "\n",
    "## Numerical instability\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac90ffc",
   "metadata": {},
   "source": [
    "## Real valued partial SVD\n",
    "\n",
    "Reference: https://j-towns.github.io/papers/svd-derivative.pdf\n",
    "\n",
    "### Forward mode\n",
    "\n",
    "The differential formulas $\\dU$, $\\dS$, and $\\dV$ in terms of $\\dA$, $\\U$, $\\S$, and $\\V$ are found from the standard chain rule (TODO check this from the differential formula).\n",
    "\n",
    "Standard chain rule\n",
    "\n",
    "$\\dA = \\dU \\S \\Vt + \\U \\dS \\Vt + \\U \\S \\dVt$\n",
    "\n",
    "Differential formulas\n",
    "\n",
    "$\\dU = \\U ( \\F \\circ [\\Ut \\dA \\V \\S + \\S \\Vt \\dAt \\U] ) + (\\I_m - \\U \\Ut ) \\dA \\V \\S^{-1}$\n",
    "\n",
    "$\\dS = \\I_k \\circ [\\Ut \\dA \\V]$\n",
    "\n",
    "$\\dV = \\V (\\F \\circ [\\S \\Ut \\dA \\V + \\Vt \\dAt \\U \\S]) + (\\I_n - \\V \\Vt) \\dAt \\U \\S^{-1}$\n",
    "\n",
    "where\n",
    "\n",
    "$F_{ij} = \\frac{1}{s_j^2 - s_i^s}, i \\neq j$\n",
    "\n",
    "$F_{ij} = 0$ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56584cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_partial(A, dA):\n",
    "    # TODO add dimension check\n",
    "    \n",
    "    U, S_vals, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=False)\n",
    "\n",
    "    S = jnp.diag(S_vals)\n",
    "    Ut = U.T\n",
    "    V = Vt.T\n",
    "    dAt = dA.T\n",
    "\n",
    "    k = S.shape[0]\n",
    "    m = U.shape[0]\n",
    "    n = Vt.shape[0]\n",
    "\n",
    "    I_k = jnp.eye(k)\n",
    "    I_m = jnp.eye(m)\n",
    "    I_n = jnp.eye(n)\n",
    "\n",
    "    S_inv = jnp.linalg.inv(S)\n",
    "\n",
    "    F_i_j = lambda i, j: lax.cond(i == j, lambda: 0., lambda: 1 / (S_vals[j]**2 - S_vals[i]**2))\n",
    "    F_fun = jax.vmap(jax.vmap(F_i_j, (None, 0)), (0, None))\n",
    "\n",
    "    indices = jnp.arange(k)\n",
    "    F = F_fun(indices, indices)\n",
    "\n",
    "    dU = U @ (F * ((Ut @ dA @ V @ S) + (S @ Vt @ dAt @ U))) + ((I_m - U @ Ut) @ dA @ V @ S_inv)\n",
    "    \n",
    "    # Note that the `I_k *` is extraneous. It zeros out the rest of the matrix besides the diagonal.\n",
    "    # We only return `dS_vals` which takes only the diagonal of `dS` anyway. \n",
    "    dS = I_k * (Ut @ dA @ V)\n",
    "    dS_vals = jnp.diagonal(dS)\n",
    "    \n",
    "    dV = V @ (F * (S @ Ut @ dA @ V + Vt @ dAt @ U @ S)) + (I_n - V @ Vt) @ dAt @ U @ S_inv\n",
    "    \n",
    "    return (U, S_vals, Vt), (dU, dS_vals, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b00e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_svd_uv(a, b):\n",
    "    (U, S, Vt), (dU, dS, dVt) = a\n",
    "    (U_, S_, Vt_), (dU_, dS_, dVt_) = b\n",
    "    \n",
    "    assert_allclose(U, U_)\n",
    "    assert_allclose(S, S_)\n",
    "    assert_allclose(Vt, Vt_)\n",
    "    assert_allclose(dU, dU_)\n",
    "    assert_allclose(dS, dS_)\n",
    "    assert_allclose(dVt, dVt_)\n",
    "\n",
    "def assert_allclose(l, r):\n",
    "    assert(jnp.allclose(jnp.array(l), jnp.array(r)))\n",
    "        \n",
    "def jax_real_valued_partial(A, dA):\n",
    "    A = jnp.array(A)\n",
    "    dA = jnp.array(dA)\n",
    "    \n",
    "    return jax.jvp(\n",
    "        lambda A: jnp.linalg.svd(A, compute_uv=True, full_matrices=False), \n",
    "        (A,), \n",
    "        (dA,)\n",
    "    )\n",
    "\n",
    "def pytorch_real_valued_partial(A, dA):\n",
    "    A = torch.tensor(A)\n",
    "    dA = torch.tensor(dA)\n",
    "    \n",
    "    return torch_jvp(lambda A: torch.linalg.svd(A, full_matrices=False), A, dA)\n",
    "    \n",
    "def check_real_valued_partial(A, dA):\n",
    "    jax_res = jax_real_valued_partial(A, dA)\n",
    "    torch_res = pytorch_real_valued_partial(A, dA)\n",
    "    res = svd_jvp_real_valued_partial(jnp.array(A), jnp.array(dA))\n",
    "    \n",
    "    print(jax_res[0][0])\n",
    "    print(torch_res[0][0])\n",
    "    \n",
    "    print('*********')\n",
    "    \n",
    "    print(jax_res[0][1])\n",
    "    print(torch_res[0][1])\n",
    "    \n",
    "    print('********')\n",
    "    \n",
    "    print(jax_res[0][2])\n",
    "    print(torch_res[0][2])\n",
    "    \n",
    "    \n",
    "    print('**********')\n",
    "    print('**********')\n",
    "    print('**********')\n",
    "\n",
    "    print(jax_res[1][0])\n",
    "    print(torch_res[1][0])\n",
    "    print(res[1][0])\n",
    "    \n",
    "    print('*********')\n",
    "    \n",
    "    print(jax_res[1][1])\n",
    "    print(torch_res[1][1])\n",
    "    print(res[1][1])\n",
    "    \n",
    "    print('********')\n",
    "    \n",
    "    print(jax_res[1][2])\n",
    "    print(torch_res[1][2])\n",
    "    print(res[1][2])\n",
    "    \n",
    "    return\n",
    "    \n",
    "    assert_svd_uv(jax_res, torch_res)\n",
    "    assert_svd_uv(jax_res, res)\n",
    "\n",
    "A = [[1., 2., 3.], [5., 4., 3.], [10., 15., 12.]]\n",
    "dA = [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]\n",
    "\n",
    "# check_real_valued_partial(A, dA)\n",
    "\n",
    "A = jnp.array(A)\n",
    "dA = jnp.array(dA)\n",
    "\n",
    "U, S, Vt = jnp.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2a9f7495",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = S\n",
    "s_zeros = (s == 0).astype(s.dtype)\n",
    "s_inv = 1 / (s + s_zeros) - s_zeros\n",
    "s_inv_mat = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')(s_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c460a3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.04355856, 0.44321346, 1.0791255 ], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "946220fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.04355856, 0.        , 0.        ],\n",
       "             [0.        , 0.44321346, 0.        ],\n",
       "             [0.        , 0.        , 1.0791255 ]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_inv_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f3cdc997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.957602   0.         0.       ]\n",
      " [ 0.         2.2562492  0.       ]\n",
      " [ 0.         0.         0.9266763]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.04355856, 0.        , 0.        ],\n",
       "             [0.        , 0.44321346, 0.        ],\n",
       "             [0.        , 0.        , 1.0791255 ]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = jnp.diag(S)\n",
    "print(S)\n",
    "S_inv = jnp.linalg.inv(S)\n",
    "S_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc7a36",
   "metadata": {},
   "source": [
    "SVD's reverse mode autodiff is defined by the formula\n",
    "\n",
    "$$\n",
    "\\newcommand{\\gAa}{{[\\U (\\F \\circ [\\Ut \\gU - \\gUt \\U]) \\S + (\\I_m - \\U \\Ut) \\gU \\S^{-1} ] \\Vt}}\n",
    "\\newcommand{\\gAb}{{\\U (\\I_k \\circ \\gS ) \\Vt}}\n",
    "\\newcommand{\\gAc}{{\\U [\\S (\\F \\circ [\\Vt \\gV - \\gVt \\V]) \\Vt + \\S^{-1} \\gVt (\\I_n - \\V \\Vt)]}}\n",
    "\\gA = \\gAa \\newline + \\gAb \\newline + \\gAc\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp(A, U, S, Vh, gU, gS, gVh):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d952c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1.,2.], [5., 2.], [10., 23.], [10., 23.]])\n",
    "\n",
    "U, S, Vh = np.linalg.svd(A)\n",
    "\n",
    "print(U)\n",
    "print(S)\n",
    "print(Vh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72312621",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "SVD real differentiability:\n",
    "- https://j-towns.github.io/papers/svd-derivative.pdf\n",
    "- https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "- https://arxiv.org/pdf/1509.07838.pdf\n",
    "\n",
    "SVD complex differentiability:\n",
    "- https://arxiv.org/pdf/1909.02659.pdf\n",
    "- https://giggleliu.github.io/2019/04/02/einsumbp.html\n",
    "\n",
    "Existing implementations:\n",
    "\n",
    "Jax forward:\n",
    "- https://github.com/google/jax/blob/2a00533e3e686c1c9d7dfe9ed2a3b19217cfe76f/jax/_src/lax/linalg.py#L1578\n",
    "- Jax only implements the forward rule because jax can derive the backward rule from the forward rule and vice versa.\n",
    "\n",
    "Pytorch forward:\n",
    "- https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3122\n",
    "\n",
    "Pytorch backward:\n",
    "- https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3228\n",
    "\n",
    "Tensorflow forward:\n",
    "- https://github.com/tensorflow/tensorflow/blob/bbe41abdcb2f7e923489bfa21cfb546b6022f330/tensorflow/python/ops/linalg_grad.py#L815\n",
    "\n",
    "General complex differentiability:\n",
    "- https://mediatum.ub.tum.de/doc/631019/631019.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "def f(A, B):\n",
    "    return jnp.trace(A @ B)\n",
    "\n",
    "A = jnp.array([[1, 2], [3, 4]], dtype=float)\n",
    "B = jnp.array([[1,2,3], [4,5,6]], dtype=float)\n",
    "\n",
    "print(f(A, B))\n",
    "\n",
    "jax.jacfwd(f, argnums=(0, 1))(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import custom_jvp\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# f :: a -> b\n",
    "@custom_jvp\n",
    "def f(x):\n",
    "    return jnp.dot(x, x)\n",
    "\n",
    "# f_jvp :: (a, T a) -> (b, T b)\n",
    "def f_jvp(primals, tangents):\n",
    "    x, = primals\n",
    "    t, = tangents\n",
    "    return f(x), 2 * x @ t\n",
    "\n",
    "f.defjvp(f_jvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c648956",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.grad(f)(jnp.array([1.,2.,4.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e11376",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.make_jaxpr(jax.vjp(f, jnp.array([1.,2.,3.]))[1])(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.make_jaxpr(jax.vjp(lambda x: x @ x, jnp.array([1.,2.,3.]))[1])(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88158638",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.jacrev(f)(jnp.array([1.,2.,3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21988f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.jacrev(lambda x: x @ x)(jnp.array([1., 2., 3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * jnp.array([1.,2.])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
