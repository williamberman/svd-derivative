{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6b819698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import jax\n",
    "import torch\n",
    "import torch.autograd.functional as F\n",
    "torch_jvp = F.jvp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05bc7a9",
   "metadata": {},
   "source": [
    "## Cases\n",
    "$$\n",
    "\\newcommand{\\d}{{\\mathrm{d}}}\n",
    "\\newcommand{\\A}{{\\mathbf{A}}}\n",
    "\\newcommand{\\U}{{\\mathbf{U}}}\n",
    "\\newcommand{\\S}{{\\mathbf{S}}}\n",
    "\\newcommand{\\V}{{\\mathbf{V}}}\n",
    "\\newcommand{\\F}{{\\mathbf{F}}}\n",
    "\\newcommand{\\I}{{\\mathbf{I}}}\n",
    "\\newcommand{\\dA}{{\\d\\A}}\n",
    "\\newcommand{\\dU}{{\\d\\U}}\n",
    "\\newcommand{\\dS}{{\\d\\S}}\n",
    "\\newcommand{\\dV}{{\\d\\V}}\n",
    "\\newcommand{\\Ut}{{\\U^{\\top}}}\n",
    "\\newcommand{\\Vt}{{\\V^{\\top}}}\n",
    "\\newcommand{\\Vh}{{\\V^{H}}}\n",
    "\\newcommand{\\dAt}{{\\dA^{\\top}}}\n",
    "\\newcommand{\\dVt}{{\\dV^{\\top}}}\n",
    "\\newcommand{\\gA}{{\\overline{\\A}}}\n",
    "\\newcommand{\\gU}{{\\overline{\\U}}}\n",
    "\\newcommand{\\gUt}{{\\gU^{\\top}}}\n",
    "\\newcommand{\\gS}{{\\overline{\\S}}}\n",
    "\\newcommand{\\gSt}{{\\gS^{\\top}}}\n",
    "\\newcommand{\\gV}{{\\overline{\\V}}}\n",
    "\\newcommand{\\gVt}{{\\gV^{\\top}}}\n",
    "$$\n",
    "The derivative of the SVD operation is determined by\n",
    "\n",
    "1. Computing the full SVD vs the \"thin\"/\"partial\" SVD\n",
    "2. Computing the complete factorization $\\U\\S\\Vh$ vs computing just the singular values $\\S$\n",
    "3. complex vs real inputs\n",
    "\n",
    "These cases create 8 different cases for the SVD derivative each with separate differential formulas for forward mode AD update and adjoint formula for the reverse mode AD update.\n",
    "\n",
    "## Numerical instability\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac90ffc",
   "metadata": {},
   "source": [
    "## Real valued partial SVD\n",
    "\n",
    "Reference: https://j-towns.github.io/papers/svd-derivative.pdf\n",
    "\n",
    "### Forward mode\n",
    "\n",
    "The differential formulas $\\dU$, $\\dS$, and $\\dV$ in terms of $\\dA$, $\\U$, $\\S$, and $\\V$ are found from the standard chain rule (TODO check this from the differential formula).\n",
    "\n",
    "Standard chain rule\n",
    "\n",
    "$\\dA = \\dU \\S \\Vt + \\U \\dS \\Vt + \\U \\S \\dVt$\n",
    "\n",
    "Differential formulas\n",
    "\n",
    "$\\dU = \\U ( \\F \\circ [\\Ut \\dA \\V \\S + \\S \\Vt \\dAt \\U] ) + (\\I_m - \\U \\Ut ) \\dA \\V \\S^{-1}$\n",
    "\n",
    "$\\dS = \\I_k \\circ [\\Ut \\dA \\V]$\n",
    "\n",
    "$\\dV = \\V (\\F \\circ [\\S \\Ut \\dA \\V + \\Vt \\dAt \\U \\S]) + (\\I_n - \\V \\Vt) \\dAt \\U \\S^{-1}$\n",
    "\n",
    "where\n",
    "\n",
    "$F_{ij} = \\frac{1}{s_j^2 - s_i^s}, i \\neq j$\n",
    "\n",
    "$F_{ij} = 0$ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56584cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_partial(A, dA):\n",
    "    # TODO add dimension check\n",
    "    \n",
    "    U, S_vals, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=False)\n",
    "\n",
    "    S = jnp.diag(S_vals)\n",
    "    Ut = U.T\n",
    "    V = Vt.T\n",
    "    dAt = dA.T\n",
    "\n",
    "    k = S.shape[0]\n",
    "    m = U.shape[0]\n",
    "    n = Vt.shape[0]\n",
    "\n",
    "    I_k = jnp.eye(k)\n",
    "    I_m = jnp.eye(m)\n",
    "    I_n = jnp.eye(n)\n",
    "\n",
    "    S_inv = jnp.linalg.inv(S)\n",
    "\n",
    "    F_i_j = lambda i, j: lax.cond(i == j, lambda: 0., lambda: S_vals[i] * S_vals[j])\n",
    "    F_fun = jax.vmap(jax.vmap(F_i_j, (0, None)), (None, 0))\n",
    "\n",
    "    indices = jnp.arange(k)\n",
    "    F = F_fun(indices, indices)\n",
    "\n",
    "    dU = U @ (F * (Ut @ dA @ V @ S + S @ Vt @ dAt @ U)) + (I_m - U @ Ut) @ dA @ V @ S_inv\n",
    "    dS = I_k * (U.T @ dA @ V)\n",
    "    dV = V @ (F * (S @ Ut @ dA @ V + Vt @ dAt @ U @ S)) + (I_n - V @ Vt) @ dAt @ U @ S_inv\n",
    "    \n",
    "    dS_vals = jnp.diagonal(dS)\n",
    "    \n",
    "    return (U, S_vals, Vt), (dU, dS_vals, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "257a2195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [ 5.,  4.,  3.],\n",
       "        [10., 15., 12.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1., 2., 3.], [5., 4., 3.], [10., 15., 12.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7b00e096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[-0.1524,  0.4672, -0.8709],\n",
       "          [-0.2956, -0.8624, -0.4109],\n",
       "          [-0.9431,  0.1949,  0.2695]]),\n",
       "  tensor([22.9576,  2.2562,  0.9267]),\n",
       "  tensor([[-0.4818, -0.6810, -0.5515],\n",
       "          [-0.8404,  0.1808,  0.5110],\n",
       "          [-0.2483,  0.7096, -0.6594]])),\n",
       " (tensor([[-0.0589, -0.0750, -0.0299],\n",
       "          [-0.0451, -0.0318,  0.0992],\n",
       "          [ 0.0237,  0.0390,  0.0545]]),\n",
       "  tensor([2.3847, 0.0298, 0.2004]),\n",
       "  tensor([[-0.0126,  0.0126, -0.0045],\n",
       "          [-0.0083,  0.0456, -0.0298],\n",
       "          [ 0.0527,  0.0005, -0.0193]])))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_real_valued_partial(A, dA):\n",
    "    (U, S, Vt), (dU, dS, dVt) = jax.jvp(\n",
    "        lambda A: jnp.linalg.svd(A, compute_uv=True, full_matrices=False), \n",
    "        (A,), \n",
    "        (dA,)\n",
    "    )\n",
    "    (U_, S_, Vt_), (dU_, dS_, dVt_) = svd_jvp_real_valued_partial(A, dA)\n",
    "    \n",
    "    print(dU)\n",
    "    print(dU_)\n",
    "    \n",
    "    assert(jnp.allclose(U, U_))\n",
    "    assert(jnp.allclose(S, S_))\n",
    "    assert(jnp.allclose(Vt, Vt_))\n",
    "    assert(jnp.allclose(dU, dU_))\n",
    "    assert(jnp.allclose(dS, dS_))\n",
    "    assert(jnp.allclose(dVt, dVt_))\n",
    "\n",
    "A_ = [[1., 2., 3.], [5., 4., 3.], [10., 15., 12.]]\n",
    "\n",
    "A_jax = jnp.array(A_)\n",
    "dA_jax = jnp.ones_like(A)\n",
    "\n",
    "A_torch = torch.tensor(A_)\n",
    "dA_torch = torch.ones_like(A_torch)\n",
    "\n",
    "torch_jvp(lambda A: torch.linalg.svd(A, full_matrices=False), A_torch, dA_torch)\n",
    "\n",
    "# check_real_valued_partial(A, dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5fefdd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CompiledFunction of <function allclose at 0x7facef2aa1f0>>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc7a36",
   "metadata": {},
   "source": [
    "SVD's reverse mode autodiff is defined by the formula\n",
    "\n",
    "$$\n",
    "\\newcommand{\\gAa}{{[\\U (\\F \\circ [\\Ut \\gU - \\gUt \\U]) \\S + (\\I_m - \\U \\Ut) \\gU \\S^{-1} ] \\Vt}}\n",
    "\\newcommand{\\gAb}{{\\U (\\I_k \\circ \\gS ) \\Vt}}\n",
    "\\newcommand{\\gAc}{{\\U [\\S (\\F \\circ [\\Vt \\gV - \\gVt \\V]) \\Vt + \\S^{-1} \\gVt (\\I_n - \\V \\Vt)]}}\n",
    "\\gA = \\gAa \\newline + \\gAb \\newline + \\gAc\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32dbfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp(A, U, S, Vh, gU, gS, gVh):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d952c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06249881 -0.0248252  -0.70550606 -0.70550606]\n",
      " [-0.10831214 -0.99361751  0.02227914  0.02227914]\n",
      " [-0.70155626  0.07780729  0.52970552 -0.47029448]\n",
      " [-0.70155626  0.07780729 -0.47029448  0.52970552]]\n",
      "[35.74646973  3.76694863]\n",
      "[[-0.40941623 -0.91234772]\n",
      " [-0.91234772  0.40941623]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1.,2.], [5., 2.], [10., 23.], [10., 23.]])\n",
    "\n",
    "U, S, Vh = np.linalg.svd(A)\n",
    "\n",
    "print(U)\n",
    "print(S)\n",
    "print(Vh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72312621",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "SVD real differentiability:\n",
    "- https://j-towns.github.io/papers/svd-derivative.pdf\n",
    "- https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "- https://arxiv.org/pdf/1509.07838.pdf\n",
    "\n",
    "SVD complex differentiability:\n",
    "- https://arxiv.org/pdf/1909.02659.pdf\n",
    "- https://giggleliu.github.io/2019/04/02/einsumbp.html\n",
    "\n",
    "Existing implementations:\n",
    "\n",
    "Jax forward:\n",
    "- https://github.com/google/jax/blob/2a00533e3e686c1c9d7dfe9ed2a3b19217cfe76f/jax/_src/lax/linalg.py#L1578\n",
    "- Jax only implements the forward rule because jax can derive the backward rule from the forward rule and vice versa.\n",
    "\n",
    "Pytorch forward:\n",
    "- https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3122\n",
    "\n",
    "Pytorch backward:\n",
    "- https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3228\n",
    "\n",
    "Tensorflow forward:\n",
    "- https://github.com/tensorflow/tensorflow/blob/bbe41abdcb2f7e923489bfa21cfb546b6022f330/tensorflow/python/ops/linalg_grad.py#L815\n",
    "\n",
    "General complex differentiability:\n",
    "- https://mediatum.ub.tum.de/doc/631019/631019.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b9f6485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[1., 4.],\n",
       "              [2., 5.]], dtype=float32),\n",
       " DeviceArray([[1., 3., 0.],\n",
       "              [2., 4., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "def f(A, B):\n",
    "    return jnp.trace(A @ B)\n",
    "\n",
    "A = jnp.array([[1, 2], [3, 4]], dtype=float)\n",
    "B = jnp.array([[1,2,3], [4,5,6]], dtype=float)\n",
    "\n",
    "print(f(A, B))\n",
    "\n",
    "jax.jacfwd(f, argnums=(0, 1))(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3db5e403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.f_jvp(primals, tangents)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import custom_jvp\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# f :: a -> b\n",
    "@custom_jvp\n",
    "def f(x):\n",
    "    return jnp.dot(x, x)\n",
    "\n",
    "# f_jvp :: (a, T a) -> (b, T b)\n",
    "def f_jvp(primals, tangents):\n",
    "    x, = primals\n",
    "    t, = tangents\n",
    "    return f(x), 2 * x @ t\n",
    "\n",
    "f.defjvp(f_jvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8c648956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 4., 8.], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(f)(jnp.array([1.,2.,4.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b5e11376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22ma\u001b[35m:f32[3]\u001b[39m; b\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mc\u001b[35m:f32[3]\u001b[39m = dot_general[\n",
       "      dimension_numbers=(((), ()), ((), ()))\n",
       "      precision=None\n",
       "      preferred_element_type=None\n",
       "    ] b a\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(c,) }"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(jax.vjp(f, jnp.array([1.,2.,3.]))[1])(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "810d1719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22ma\u001b[35m:f32[3]\u001b[39m; b\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mc\u001b[35m:f32[3]\u001b[39m = dot_general[\n",
       "      dimension_numbers=(((), ()), ((), ()))\n",
       "      precision=None\n",
       "      preferred_element_type=None\n",
       "    ] b a\n",
       "    d\u001b[35m:f32[3]\u001b[39m = dot_general[\n",
       "      dimension_numbers=(((), ()), ((), ()))\n",
       "      precision=None\n",
       "      preferred_element_type=None\n",
       "    ] b a\n",
       "    e\u001b[35m:f32[3]\u001b[39m = add_any d c\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(e,) }"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(jax.vjp(lambda x: x @ x, jnp.array([1.,2.,3.]))[1])(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "88158638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 4., 6.], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jacrev(f)(jnp.array([1.,2.,3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f21988f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 4., 6.], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jacrev(lambda x: x @ x)(jnp.array([1., 2., 3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41a6958a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 4.], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * jnp.array([1.,2.])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
