{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b819698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 13:21:06.768817: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-09-09 13:21:06.785259: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import jax\n",
    "from jax import random\n",
    "import torch\n",
    "import torch.autograd.functional as TF\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "torch_jvp = TF.jvp\n",
    "torch_vjp = TF.vjp\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05bc7a9",
   "metadata": {},
   "source": [
    "## Cases\n",
    "$$\n",
    "\\newcommand{\\tr}{{\\mathrm{tr}}}\n",
    "\\newcommand{\\d}{{\\mathrm{d}}}\n",
    "\\newcommand{\\A}{{\\mathbf{A}}}\n",
    "\\newcommand{\\U}{{\\mathbf{U}}}\n",
    "\\renewcommand{\\S}{{\\mathbf{S}}}\n",
    "\\newcommand{\\V}{{\\mathbf{V}}}\n",
    "\\newcommand{\\F}{{\\mathbf{F}}}\n",
    "\\newcommand{\\I}{{\\mathbf{I}}}\n",
    "\\renewcommand{\\P}{{\\mathbf{P}}}\n",
    "\\newcommand{\\D}{{\\mathbf{D}}}\n",
    "\\newcommand{\\dA}{{\\d\\A}}\n",
    "\\newcommand{\\dU}{{\\d\\U}}\n",
    "\\newcommand{\\dS}{{\\d\\S}}\n",
    "\\newcommand{\\dV}{{\\d\\V}}\n",
    "\\newcommand{\\dP}{{\\d\\P}}\n",
    "\\newcommand{\\dD}{{\\d\\D}}\n",
    "\\newcommand{\\Ut}{{\\U^{\\top}}}\n",
    "\\newcommand{\\Vt}{{\\V^{\\top}}}\n",
    "\\newcommand{\\Vh}{{\\V^{H}}}\n",
    "\\newcommand{\\dAt}{{\\dA^{\\top}}}\n",
    "\\newcommand{\\dVt}{{\\dV^{\\top}}}\n",
    "\\newcommand{\\gA}{{\\overline{\\A}}}\n",
    "\\newcommand{\\gAt}{{\\gA^{\\top}}}\n",
    "\\newcommand{\\gU}{{\\overline{\\U}}}\n",
    "\\newcommand{\\gUt}{{\\gU^{\\top}}}\n",
    "\\newcommand{\\gS}{{\\overline{\\S}}}\n",
    "\\newcommand{\\gSt}{{\\gS^{\\top}}}\n",
    "\\newcommand{\\gV}{{\\overline{\\V}}}\n",
    "\\newcommand{\\gVt}{{\\gV^{\\top}}}\n",
    "\\newcommand{\\K}{{\\mathbf{K}}}\n",
    "\\newcommand{\\Ku}{{\\K_u}}\n",
    "\\newcommand{\\Kut}{{\\Ku^{\\top}}}\n",
    "\\newcommand{\\Vm}{{\\V_m}}\n",
    "\\newcommand{\\Vmt}{{\\Vm^{\\top}}}\n",
    "\\newcommand{\\Kv}{{\\K_v}}\n",
    "\\newcommand{\\Kvt}{{\\Kv^{\\top}}}\n",
    "\\newcommand{\\Vr}{{\\V_r}}\n",
    "\\newcommand{\\Vrt}{{\\Vr^{\\top}}}\n",
    "\\newcommand{\\Ft}{{\\F^{\\top}}}\n",
    "\\newcommand{\\gVm}{{\\gV_m}}\n",
    "\\newcommand{\\gVmt}{{\\gVm^{\\top}}}\n",
    "\\newcommand{\\gVr}{{\\gV_r}}\n",
    "\\newcommand{\\Kc}{{\\K_c}}\n",
    "\\newcommand{\\Kch}{{\\K_c^{\\dagger}}}\n",
    "\\newcommand{\\Vh}{{\\V^{\\dagger}}}\n",
    "\\newcommand{\\Uh}{{\\U^{\\dagger}}}\n",
    "\\newcommand{\\Kuh}{{\\Ku^{\\dagger}}}\n",
    "\\newcommand{\\Kvh}{{\\Kv^{\\dagger}}}\n",
    "\\newcommand{\\Vmh}{{\\Vm^{\\dagger}}}\n",
    "\\newcommand{\\gVmh}{{\\gVm^{\\dagger}}}\n",
    "\\newcommand{\\Vrh}{{\\Vr^{\\dagger}}}\n",
    "\\newcommand{\\gVh}{{\\gV^{\\dagger}}}\n",
    "\\newcommand{\\termu}{{\\mathrm{term}_U}}\n",
    "\\newcommand{\\terms}{{\\mathrm{term}_S}}\n",
    "\\newcommand{\\termv}{{\\mathrm{term}_V}}\n",
    "\\newcommand{\\termc}{{\\mathrm{term}_C}}\n",
    "$$\n",
    "The derivative of the SVD operation is determined by\n",
    "\n",
    "1. Computing the full SVD vs the \"thin\"/\"partial\" SVD\n",
    "2. Complex vs real inputs\n",
    "3. Computing the complete factorization $\\U\\S\\Vh$ vs computing only the singular values $\\S$\n",
    "\n",
    "Computing only the singular values does not change the differential or gradient formulas and can be considered a side case of the factorization.\n",
    "\n",
    "##### Algebra\n",
    "\n",
    "I'm omitting most of the algebra, particularly in deriving the backward pass of the full case. Most of the algebra in the backwards case is massaging the chain rule to move the $\\dA$ such that $\\tr(\\gAt \\dA) = \\tr(X \\dA)$ and $ \\gA = X^{\\top}$. The algebra mostly breaks down to using equation 65 of [Old and New Matrix Algebra Useful for Statistics](https://tminka.github.io/papers/matrix/minka-matrix.pdf) to pull the $\\dA$ out of the element wise product and then using the [cyclic property of trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)) to rotate the $\\dA$ to the right hand side. If $\\dA$ is transposed, apply the [trace of a product](https://en.wikipedia.org/wiki/Trace_(linear_algebra)) rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac90ffc",
   "metadata": {},
   "source": [
    "## Real valued partial SVD\n",
    "\n",
    "Reference: https://j-towns.github.io/papers/svd-derivative.pdf\n",
    "\n",
    "### Forward mode\n",
    "\n",
    "The differential formulas $\\dU$, $\\dS$, and $\\dV$ in terms of $\\dA$, $\\U$, $\\S$, and $\\V$ are found from the chain rule.\n",
    "\n",
    "##### Chain rule\n",
    "\n",
    "$\\dA = \\dU \\S \\Vt + \\U \\dS \\Vt + \\U \\S \\dVt$\n",
    "\n",
    "TODO(will) - Is this really the chain rule? Deriving this is a little funky because you end up taking partial derivatives with respect to matrices which I'm not sure how to define. The rule for expressing the differential and the product is in 2.1/2.2 https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "\n",
    "##### Differential formulas\n",
    "\n",
    "$\\dU = \\U ( \\F \\circ [\\Ut \\dA \\V \\S + \\S \\Vt \\dAt \\U] ) + (\\I_m - \\U \\Ut ) \\dA \\V \\S^{-1}$\n",
    "\n",
    "$\\dS = \\I_k \\circ [\\Ut \\dA \\V]$\n",
    "\n",
    "$\\dV = \\V (\\F \\circ [\\S \\Ut \\dA \\V + \\Vt \\dAt \\U \\S]) + (\\I_n - \\V \\Vt) \\dAt \\U \\S^{-1}$\n",
    "\n",
    "where\n",
    "\n",
    "$F_{ij} = \\frac{1}{s_j^2 - s_i^2}, i \\neq j$\n",
    "\n",
    "$F_{ij} = 0$ otherwise\n",
    "\n",
    "$k$ is the rank of $\\A$\n",
    "\n",
    "$\\U$ is $m \\times k$\n",
    "\n",
    "$\\S$ is $k \\times k$\n",
    "\n",
    "$\\Vt$ is $k \\times n$\n",
    "\n",
    "##### Numerical instability\n",
    "Both $\\S^{-1}$ and $\\F$ cause numerical instability in $\\dU$ and $\\dV$.  \n",
    "\n",
    "$\\S^{-1}$ causes numerical instability with zero or very small singular values. The inverse of a diagonal matrix is the element wise reciprocals of the inverting matrix's diagonal. Zero singular values will cause $\\S^{-1}$ to have infinite elements. Very small singular values will cause $\\S^{-1}$ to have very large floating point elements.\n",
    "\n",
    "$\\F$ divides by the difference of all pairs of singular values. Repeated singular values will cause infinite elements of $\\F$. Close singular values will cause very large floating point elements of $\\F$.  \n",
    "\n",
    "##### Computing only singular values\n",
    "Only compute the differential for $\\S$. $\\dU$ and $\\dV$ are ignored. Because $\\dU$ and $\\dV$ are not computed, $\\F$ and $\\S^{-1}$ are not needed, so there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56584cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_partial(A, dA):    \n",
    "    U, S_vals, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=False)\n",
    "\n",
    "    S = jnp.diag(S_vals)\n",
    "    Ut = U.T\n",
    "    V = Vt.T\n",
    "    dAt = dA.T\n",
    "\n",
    "    k = S.shape[0]\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "\n",
    "    I_k = jnp.eye(k)\n",
    "    I_m = jnp.eye(m)\n",
    "    I_n = jnp.eye(n)\n",
    "\n",
    "    S_inv = jnp.linalg.inv(S)\n",
    "\n",
    "    F_ = F(S_vals, k)\n",
    "\n",
    "    dU = U @ (F_ * ((Ut @ dA @ V @ S) + (S @ Vt @ dAt @ U))) + ((I_m - U @ Ut) @ dA @ V @ S_inv)\n",
    "    \n",
    "    # Note that the `I_k *` is extraneous. It zeros out the rest of the matrix besides the diagonal.\n",
    "    # We only return `dS_vals` which takes only the diagonal of `dS` anyway. \n",
    "    dS = I_k * (Ut @ dA @ V)\n",
    "    dS_vals = jnp.diagonal(dS)\n",
    "    \n",
    "    dV = V @ (F_ * (S @ Ut @ dA @ V + Vt @ dAt @ U @ S)) + (I_n - V @ Vt) @ dAt @ U @ S_inv\n",
    "    \n",
    "    return (U, S_vals, Vt), (dU, dS_vals, dV.T)\n",
    "\n",
    "def F(S_vals, k):\n",
    "    F_i_j = lambda i, j: lax.cond(i == j, lambda: 0., lambda: 1 / (S_vals[j]**2 - S_vals[i]**2))\n",
    "    F_fun = jax.vmap(jax.vmap(F_i_j, (None, 0)), (0, None))\n",
    "\n",
    "    indices = jnp.arange(k)\n",
    "    F_ = F_fun(indices, indices)\n",
    "    \n",
    "    return F_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b00e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_svd_uv(a, b):\n",
    "    (U, S, Vt), (dU, dS, dVt) = a\n",
    "    (U_, S_, Vt_), (dU_, dS_, dVt_) = b\n",
    "    \n",
    "    assert_allclose(U, U_)\n",
    "    assert_allclose(S, S_)\n",
    "    assert_allclose(Vt, Vt_)\n",
    "    assert_allclose(dU, dU_)\n",
    "    assert_allclose(dS, dS_)\n",
    "    assert_allclose(dVt, dVt_)\n",
    "\n",
    "def check_allclose(l, r):\n",
    "    return jnp.allclose(jnp.array(l), jnp.array(r), atol=1e-04)\n",
    "\n",
    "def assert_allclose(l, r):\n",
    "    assert(check_allclose(l, r))\n",
    "        \n",
    "def jax_real_valued_partial(A, dA):\n",
    "    A = jnp.array(A)\n",
    "    dA = jnp.array(dA)\n",
    "    \n",
    "    return jax.jvp(\n",
    "        lambda A: jnp.linalg.svd(A, compute_uv=True, full_matrices=False), \n",
    "        (A,), \n",
    "        (dA,)\n",
    "    )\n",
    "\n",
    "def pytorch_real_valued_partial(A, dA):\n",
    "    A = torch.tensor(A)\n",
    "    dA = torch.tensor(dA)\n",
    "    \n",
    "    return torch_jvp(lambda A: torch.linalg.svd(A, full_matrices=False), A, dA)\n",
    "    \n",
    "def check_real_valued_partial(A, dA):\n",
    "    jax_res = jax_real_valued_partial(A, dA)\n",
    "    torch_res = pytorch_real_valued_partial(A, dA)\n",
    "    res = svd_jvp_real_valued_partial(jnp.array(A), jnp.array(dA))\n",
    "    \n",
    "    # example == jax == torch\n",
    "    assert_svd_uv(jax_res, torch_res)\n",
    "    assert_svd_uv(jax_res, res)\n",
    "    \n",
    "    return jax_res, torch_res, res\n",
    "\n",
    "def check_real_valued_partial_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "    dA = jnp.ones_like(A)\n",
    "            \n",
    "    A = A.tolist()\n",
    "    dA = dA.tolist()\n",
    "    \n",
    "    return A, dA\n",
    "  \n",
    "def check_real_valued_partials():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_partial_args(subkey, m, n)\n",
    "            check_real_valued_partial(*args)\n",
    "\n",
    "check_real_valued_partials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc7a36",
   "metadata": {},
   "source": [
    "### Reverse mode\n",
    "\n",
    "##### Chain rule\n",
    "$ \\tr(\\gAt \\dA) = \\tr(\\gUt \\dU) + \\tr(\\gSt \\dS) + \\tr(\\gVt \\dV) $\n",
    "\n",
    "##### Gradient formula\n",
    "\n",
    "The formula for A's gradient has terms $\\termu$, $\\terms$, and $\\termv$ found from the respective trace terms in the chain rule\n",
    "\n",
    "$\\gA = \\termu + \\terms + \\termv$\n",
    "\n",
    "$\\termu = \\U [\\Ku + \\Kut] \\S \\Vt + [ \\I_m - \\U \\Ut ] \\gU \\S^{-1} \\Vt$\n",
    "\n",
    "$\\terms = \\U (\\I_k \\circ \\gS ) \\Vt$\n",
    "\n",
    "$\\termv = \\U \\S [\\Kv + \\Kvt] \\Vt + \\U \\S^{-1} \\gVt (\\I_n - \\V \\Vt)$\n",
    "\n",
    "$\\Ku = \\F \\circ ( \\Ut \\gU ) $\n",
    "\n",
    "$\\Kv = \\F \\circ ( \\Vt \\gV ) $\n",
    "\n",
    "Note that the second terms in $\\termu$ and $\\termv$ are to account for when $\\A$ is non-square. They will both be $0$ when $A$ is square due to $\\U \\Ut$ and $\\V \\Vt$ both being equal to identity matrices because $\\U$ and $\\V$ are individually composed of orthogonal vectors. For a non-square $A$, one of the two terms will be $0$ corresponding to which of $\\U$ or $\\V$ is rank-deficient. Without loss of generality, we can omit one of the two second terms. I.e. for a $m \\times n$ input, only compute the gradient for matrices where $m \\lt n$ omitting the second term on $\\termu$ because only $\\V$ will ever be rank deficient. For matrices where $m \\gt n$, we can compute the gradient of the transpose and then transpose the resulting gradient.   \n",
    "\n",
    "##### Numerical instability\n",
    "$\\S^{-1}$ and $\\F$ cause numerical instability in $\\gA$ through $\\termu$ and $\\termv$. The explanation is the same as in the forward mode.\n",
    "\n",
    "##### Computing only singular values\n",
    "When only computing singular values, no $\\U$ or $\\Vt$ were produced to impact a resulting objective function. $\\gU$ and $\\gVt$ must then be zero, and so are $\\termu$ and $\\termv$. This means that $\\gA = \\terms$, and there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32dbfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_partial(A, U, S_vals, Vt, gU, gS_vals, gVt):\n",
    "    S = jnp.diag(S_vals)\n",
    "    gS = jnp.diag(gS_vals)\n",
    "    \n",
    "    k = S.shape[0]\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    I_m = jnp.eye(m)\n",
    "    I_k = jnp.eye(k)\n",
    "    I_n = jnp.eye(n)\n",
    "    \n",
    "    V = Vt.T\n",
    "    Ut = U.T\n",
    "    gUt = gU.T\n",
    "    gV = gVt.T\n",
    "    \n",
    "    S_inv = jnp.linalg.inv(S)\n",
    "    \n",
    "    F_ = F(S_vals, k)\n",
    "    \n",
    "    ############### term_U ################\n",
    "    \n",
    "    K_u = F_ * (Ut @ gU)\n",
    "    K_ut = K_u.T\n",
    "    \n",
    "    term_U = U @ (K_u + K_ut) @ S @ Vt + (I_m - U @ Ut) @ gU @ S_inv @ Vt\n",
    "    \n",
    "    ########################################\n",
    "    \n",
    "    ############## term_S ################\n",
    "    \n",
    "    term_S = U @ (I_k * gS) @ Vt\n",
    "    \n",
    "    #####################################\n",
    "    \n",
    "    ############# term_V ################\n",
    "    \n",
    "    K_v = F_ * (Vt @ gV)\n",
    "    K_vt = K_v.T\n",
    "    \n",
    "    term_V = U @ S @ (K_v + K_vt) @ Vt + U @ S_inv @ gVt @ (I_n - V @ Vt)\n",
    "    \n",
    "    ####################################\n",
    "    \n",
    "    gA = term_U + term_S + term_V\n",
    "    \n",
    "    return gA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08d7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_partial_(A, gU, gS, gVt):\n",
    "    A = jnp.array(A)\n",
    "    gU = jnp.array(gU)\n",
    "    gS = jnp.array(gS)\n",
    "    gVt = jnp.array(gVt)\n",
    "    \n",
    "    U, S, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=False)\n",
    "    \n",
    "    gA = svd_vjp_real_valued_partial(A, U, S, Vt, gU, gS, gVt)\n",
    "    \n",
    "    return gA\n",
    "\n",
    "def jax_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    A = jnp.array(A)\n",
    "    gU = jnp.array(gU)\n",
    "    gS = jnp.array(gS)\n",
    "    gVt = jnp.array(gVt)    \n",
    "    \n",
    "    _, vjp_fun = jax.vjp(\n",
    "        lambda A: jnp.linalg.svd(A, compute_uv=True, full_matrices=False), \n",
    "        A,\n",
    "    )\n",
    "    \n",
    "    gA, = vjp_fun((gU, gS, gVt))\n",
    "    \n",
    "    return gA\n",
    "\n",
    "def pytorch_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    A = torch.tensor(A)\n",
    "    gU = torch.tensor(gU)\n",
    "    gS = torch.tensor(gS)\n",
    "    gVt = torch.tensor(gVt)\n",
    "    \n",
    "    _, gA = torch_vjp(lambda A: torch.linalg.svd(A, full_matrices=False), A, (gU, gS, gVt))\n",
    "    \n",
    "    return gA\n",
    "    \n",
    "def check_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    jax_gA = jax_real_valued_partial_vjp(A, gU, gS, gVt)\n",
    "    torch_gA = pytorch_real_valued_partial_vjp(A, gU, gS, gVt)\n",
    "    res_gA = svd_vjp_real_valued_partial_(A, gU, gS, gVt)\n",
    "    \n",
    "    # example == jax == torch\n",
    "    assert_allclose(jax_gA, torch_gA)\n",
    "    assert_allclose(jax_gA, res_gA)\n",
    "    \n",
    "    return jax_gA, torch_gA, res_gA\n",
    "    \n",
    "def check_real_valued_partial_vjp_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "    \n",
    "    k = min(m, n)\n",
    "\n",
    "    gU = jnp.ones((m, k))\n",
    "    gS = jnp.ones(k)\n",
    "    gVt = jnp.ones((k, n))\n",
    "    \n",
    "    A = A.tolist()\n",
    "    gU = gU.tolist()\n",
    "    gS = gS.tolist()\n",
    "    gVt = gVt.tolist()\n",
    "    \n",
    "    return A, gU, gS, gVt\n",
    "\n",
    "def check_real_valued_partials_vjp():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_partial_vjp_args(subkey, m, n)\n",
    "            check_real_valued_partial_vjp(*args)\n",
    "\n",
    "            \n",
    "check_real_valued_partials_vjp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b24fa",
   "metadata": {},
   "source": [
    "## Real valued full SVD\n",
    "\n",
    "Reference: https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "\n",
    "Without loss of generality, assume rectangular inputs have more columns than rows. For inputs with less columns than rows, compute gradients and differentials for the transpose of the input and then transpose the results.  \n",
    "\n",
    "\n",
    "### Forward mode\n",
    "\n",
    "The differential formulas are found by a similar process to the partial case.\n",
    "\n",
    "##### Differential formulas\n",
    "\n",
    "$ \\dU = \\U [\\F \\circ ( \\dP_1 \\S_1 + \\S_1 \\dP_1^{\\top} ) ] $\n",
    "\n",
    "$ \\Ut \\dA \\V = \\begin{bmatrix} \\dP_1 & \\dP_2 \\end{bmatrix} $\n",
    "\n",
    "$ \\S = \\begin{bmatrix} \\S_1 & 0 \\end{bmatrix} $\n",
    "\n",
    "$ \\dP_1 $ is $m \\times m$\n",
    "\n",
    "$ \\S_1 $ is $m \\times m$\n",
    "\n",
    "---\n",
    "\n",
    "$ \\dS = \\I \\circ [\\U^{\\top} \\dA \\V ] $\n",
    "\n",
    "---\n",
    "\n",
    "$ \\dV = \\V \\dD $\n",
    "\n",
    "$ \\dD = \\begin{bmatrix} \\dD_1 & -\\dD_2 \\\\ \\dD_2^{\\top} & \\dD_3 \\end{bmatrix} $\n",
    "\n",
    "$ \\dD_1 = \\F \\circ (\\S_1 \\dP_1 + \\dP_1^{\\top} \\S_1 ) $\n",
    "\n",
    "$ \\dD_2 = \\S_1^{-1} \\dP_2 $\n",
    "\n",
    "$ \\dD_3 = \\mathbf{0} $\n",
    "\n",
    "$ \\dD $ is $ n \\times n $\n",
    "\n",
    "$ \\dD_1 $ is $ m \\times m $\n",
    "\n",
    "$ \\dD_2 $ is $ m \\times (n - m) $\n",
    "\n",
    "$ \\dD_3 $ is $ (n - m) \\times (n - m) $\n",
    "\n",
    "##### Numerical instability\n",
    "The full SVD has the same numerical instability as the partial SVD.\n",
    "\n",
    "Both $\\S_1^{-1}$ and $\\F$ cause numerical instability in $\\dU$ and $\\dV$.\n",
    "\n",
    "Additionally, we know ahead of time $\\F$ will contain divisions by zero when $|n - m| \\geq 2$ because this guarantees at least two zero valued singular values. \n",
    "\n",
    "##### Computing only singular values\n",
    "Same as in the partial SVD. Only computing the singular values ignores $\\dU$ and $\\dV$ which means there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91523b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_full(A, dA):\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    if m > n:\n",
    "        (U, S, Vt), (dU, dS, dVt) = svd_jvp_real_valued_full(A.T, dA.T)\n",
    "        return (Vt.T, S, U.T), (dVt.T, dS, dU.T)\n",
    "    \n",
    "    # m <= n\n",
    "    \n",
    "    U, S_vals, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=True)\n",
    "        \n",
    "    Ut = U.T\n",
    "    V = Vt.T\n",
    "    \n",
    "    I = fill_diagonal(jnp.zeros((m, n)), 1)\n",
    "\n",
    "    # Can directly create S_1 without creating S which has added zero columns\n",
    "    # There will be m singular values, so S_1 will be m x m.\n",
    "    S_1 = jnp.diag(S_vals)\n",
    "    S_1_inv = jnp.linalg.inv(S_1)\n",
    "    \n",
    "    # m is number singular values insted of k as in partial version\n",
    "    F_ = F(S_vals, m)\n",
    "    \n",
    "    ########## dU ###########\n",
    "\n",
    "    dP = Ut @ dA @ V\n",
    "    dP_1 = dP[:, :m]\n",
    "    dP_2 = dP[:, m:]\n",
    "    dP_1t = dP_1.T\n",
    "    \n",
    "    dU = U @ (F_ * (dP_1 @ S_1 + S_1 @ dP_1t))\n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ######### dS ##########\n",
    "    \n",
    "    dS = I * (Ut @ dA @ V)\n",
    "    dS_vals = jnp.diagonal(dS)\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    ######## dV ##########\n",
    "    \n",
    "    dD_1 = F_ * (S_1 @ dP_1 + dP_1t @ S_1)\n",
    "    dD_2 = S_1_inv @ dP_2\n",
    "    dD_3 = jnp.zeros((n-m, n-m))\n",
    "    neg_dD_2 = -dD_2\n",
    "    dD_2t = dD_2.T\n",
    "    \n",
    "    dD_top = jnp.concatenate((dD_1, neg_dD_2), axis=1)\n",
    "    dD_bottom = jnp.concatenate((dD_2t, dD_3), axis=1)\n",
    "    \n",
    "    dD = jnp.concatenate((dD_top, dD_bottom), axis=0)\n",
    "    \n",
    "    dV = V @ dD\n",
    "    \n",
    "    #####################\n",
    "    \n",
    "    return (U, S_vals, Vt), (dU, dS_vals, dV.T)\n",
    "\n",
    "# modified version of fill_diagonal because jax does not implement it.\n",
    "# https://github.com/numpy/numpy/blob/v1.23.0/numpy/lib/index_tricks.py#L779-L910\n",
    "def fill_diagonal(a, val, start=None):\n",
    "    m, n = a.shape\n",
    "    \n",
    "    end = None\n",
    "    step = a.shape[1] + 1\n",
    "    \n",
    "    if start:\n",
    "        start_row, start_col = start\n",
    "        start = start_row * n + start_col\n",
    "        a = a.flatten().at[start:end:step].set(val)\n",
    "    else:\n",
    "        a = a.flatten().at[:end:step].set(val)\n",
    "        \n",
    "    a = a.reshape((m, n))\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df01bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_full_(A, dA):\n",
    "    A = jnp.array(A)\n",
    "    dA = jnp.array(dA)\n",
    "    \n",
    "    return svd_jvp_real_valued_full(A, dA)\n",
    "    \n",
    "def pytorch_real_valued_full(A, dA):\n",
    "    A = torch.tensor(A)\n",
    "    dA = torch.tensor(dA)\n",
    "    \n",
    "    return torch_jvp(lambda A: torch.linalg.svd(A, full_matrices=True), A, dA)\n",
    "\n",
    "def check_real_valued_full(A, dA):\n",
    "    # NOTE - JAX does not implement full\n",
    "    torch_res = pytorch_real_valued_partial(A, dA)\n",
    "    res = svd_jvp_real_valued_partial(jnp.array(A), jnp.array(dA))\n",
    "    \n",
    "    assert_svd_uv(torch_res, res)\n",
    "    \n",
    "    return torch_res, res\n",
    "\n",
    "def check_real_valued_full_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "    dA = jnp.ones_like(A)\n",
    "            \n",
    "    A = A.tolist()\n",
    "    dA = dA.tolist()\n",
    "    \n",
    "    return A, dA\n",
    "  \n",
    "def check_real_valued_fulls():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_full_args(subkey, m, n)\n",
    "            check_real_valued_full(*args)\n",
    "\n",
    "check_real_valued_fulls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be40654",
   "metadata": {},
   "source": [
    "### Backward mode\n",
    "\n",
    "The backward mode for the full case is found by the same process as in the partial case, but the complete formula is not documented in the reference. \n",
    "\n",
    "##### Gradient formula\n",
    "\n",
    "$\\gA = \\termu + \\terms + \\termv$\n",
    "\n",
    "$\\termu = \\U [\\Ku + \\Kut] \\S_1 \\Vmt $\n",
    "\n",
    "$\\terms = \\U (\\I \\circ \\gS) \\Vt$\n",
    "\n",
    "$\\termv = \\U \\S_1 [\\Kv + \\Kvt] \\Vmt + \\U \\S_1^{-1} [ \\gVmt \\Vr - \\Vmt \\gVr ] \\Vrt$\n",
    "\n",
    "$\\Ku = \\F \\circ (\\Ut\\gU)$\n",
    "\n",
    "$\\Kv = \\F \\circ (\\Vmt \\gVm)$\n",
    "\n",
    "$ \\Vm $ is the left most $m$ columns of $\\V $.\n",
    "\n",
    "$ \\gVm $ is the left most $m$ columns of $\\gV $.\n",
    "\n",
    "$ \\Vr $ is the right most $n - m$ columns of $\\V$.\n",
    "\n",
    "$ \\gVr $ is the right most $n - m$ columns of $\\gV$.\n",
    "\n",
    "##### Alternate $\\termv$\n",
    "\n",
    "Using the fact that $\\I = \\Vr\\Vrt + \\Vm\\Vmt$, we can massage $\\termv$ so that it is equal to $\\termv$ from the partial SVD backward pass with one additional term.\n",
    "\n",
    "$\\termv = \\U \\S_1 [\\Kv + \\Kvt] \\Vmt + \\U \\S_1^{-1} \\gVmt [ \\I - \\Vm \\Vmt ] - \\U \\S_1^{-1} \\Vmt \\gVr \\Vrt $\n",
    "\n",
    "##### Numerical instability\n",
    "$\\S^{-1}$ and $\\F$ cause numerical instability in $\\gA$ through $\\termu$ and $\\termv$. The explanation is the same as in the forward mode.\n",
    "\n",
    "##### Computing only singular values\n",
    "When only computing singular values, no $\\U$ or $\\Vt$ were produced to impact a resulting objective function. $\\gU$ and $\\gVt$ must then be zero, and so are $\\termu$ and $\\termv$. This means that $\\gA = \\terms$, and there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1610d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_full(A, U, S_vals, Vt, gU, gS_vals, gVt):\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    if m > n:\n",
    "        gA = svd_vjp_real_valued_full(A.T, Vt.T, S_vals, U.T, gVt.T, gS_vals, gU.T)\n",
    "        return gA.T\n",
    "    \n",
    "    # m <= n\n",
    "    \n",
    "    S = fill_diagonal(jnp.zeros((m, n)), S_vals)\n",
    "    gS = fill_diagonal(jnp.zeros((m, n)), gS_vals)\n",
    "    S_1 = fill_diagonal(jnp.zeros((m, m)), S_vals)\n",
    "    S_1t = S_1.T\n",
    "    S_1_inv = jnp.linalg.inv(S_1)\n",
    "    \n",
    "    F_ = F(S_vals, m)\n",
    "    Ft = F_.T\n",
    "    \n",
    "    V = Vt.T\n",
    "    Ut = U.T\n",
    "    gSt = gS.T\n",
    "    gV = gVt.T\n",
    "    \n",
    "    Vm = V[:,:m]\n",
    "    Vr = V[:,m:]\n",
    "    Vmt = Vm.T\n",
    "    Vrt = Vr.T\n",
    "    \n",
    "    gVm = gV[:,:m]\n",
    "    gVr = gV[:,m:]\n",
    "    gVmt = gVm.T\n",
    "    \n",
    "    \n",
    "    ############ term_U ##########\n",
    "    \n",
    "    K_u = F_ * (Ut @ gU)\n",
    "    K_ut = K_u.T\n",
    "    term_U = U @ (K_u + K_ut) @ S_1 @ Vmt\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ########## term_S ###########\n",
    "    \n",
    "    I = fill_diagonal(jnp.zeros((m, n)), 1)\n",
    "    term_S = U @ (I * gS) @ Vt\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ######### term_V ############\n",
    "\n",
    "    K_v = F_ * (Vmt @ gVm)\n",
    "    K_vt = K_v.T\n",
    "    \n",
    "    term_V1 = U @ S_1t @ (K_v + K_vt) @ Vmt\n",
    "    term_V2 = U @ S_1_inv @ (gVmt @ Vr - Vmt @ gVr) @ Vrt\n",
    "    \n",
    "    term_V = term_V1 + term_V2\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    gA = term_U + term_S + term_V\n",
    "    \n",
    "    return gA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7cea8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_full_(A, gU, gS, gVt):\n",
    "    A = jnp.array(A)\n",
    "    gU = jnp.array(gU)\n",
    "    gS = jnp.array(gS)\n",
    "    gVt = jnp.array(gVt)\n",
    "    \n",
    "    U, S, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=True)\n",
    "    \n",
    "    gA = svd_vjp_real_valued_full(A, U, S, Vt, gU, gS, gVt)\n",
    "    \n",
    "    return gA\n",
    "    \n",
    "def tf_full_vjp(A, gU, gS, gVt):\n",
    "    A = tf.constant(A)\n",
    "    gU = tf.constant(gU)\n",
    "    gS = tf.constant(gS)\n",
    "    gVt = tf.constant(gVt)\n",
    "    \n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(A)\n",
    "        S, U, V = tf.linalg.svd(A, full_matrices=True, compute_uv=True)\n",
    "        Vt = tf.transpose(V)\n",
    "        \n",
    "    gA = g.gradient((S, U, Vt), A, (gS, gU, gVt))\n",
    "        \n",
    "    return gA\n",
    "\n",
    "def check_real_valued_full_vjp(A, gU, gS, gVt):\n",
    "    tf_gA = tf_full_vjp(A, gU, gS, gVt)\n",
    "    res_gA = svd_vjp_real_valued_full_(A, gU, gS, gVt)\n",
    "    \n",
    "    assert_allclose(tf_gA, res_gA)\n",
    "    \n",
    "    return tf_gA, res_gA\n",
    "\n",
    "def check_full_vjp_args(key, m, n, dtype=None):\n",
    "    # Keep trying until we get an A where the factorizations are the same\n",
    "    while True:\n",
    "        key, subkey = random.split(key)\n",
    "        A, gU, gS, gVt = check_full_vjp_args_(subkey, m, n, dtype=dtype)\n",
    "        \n",
    "        if check_full_vjp_args_same_results(A):\n",
    "            break\n",
    "\n",
    "    return key, (A, gU, gS, gVt)\n",
    "\n",
    "# NOTE(will) - tf and jax do not always give the same U and V results. \n",
    "# When U and V differ, they differ only by sign at particular points but are\n",
    "# still both valid factorizations\n",
    "def check_full_vjp_args_same_results(A):\n",
    "    tf_S, tf_U, tf_V = tf.linalg.svd(tf.constant(A), full_matrices=True, compute_uv=True)\n",
    "    tf_Vt = tf.transpose(tf_V, conjugate=True)\n",
    "        \n",
    "    jax_U, jax_S, jax_Vt = jnp.linalg.svd(jnp.array(A), full_matrices=True, compute_uv=True)\n",
    "    \n",
    "    return check_allclose(jax_U, tf_U) and check_allclose(jax_S, tf_S) and check_allclose(jax_Vt, tf_Vt)\n",
    "\n",
    "def check_full_vjp_args_(key, m, n, dtype=None):\n",
    "    A = random.normal(key, (m, n), dtype=dtype)\n",
    "\n",
    "    k = min(m, n)\n",
    "\n",
    "    gU = jnp.zeros((m, m), dtype=dtype)\n",
    "    gS = jnp.zeros(k)\n",
    "    gVt = jnp.ones((n, n), dtype=dtype)\n",
    "\n",
    "    A = A.tolist()\n",
    "    gU = gU.tolist()\n",
    "    gS = gS.tolist()\n",
    "    gVt = gVt.tolist()\n",
    "\n",
    "    return A, gU, gS, gVt\n",
    "  \n",
    "def check_real_valued_fulls_vjp():\n",
    "    key_idx = 0\n",
    "    \n",
    "    keys = [\n",
    "        [1278412471, 2182328957],\n",
    "        [2205739499, 3850766070],\n",
    "        [3971862319, 2771547813],\n",
    "        [2936998373,  532740107],\n",
    "        [  41787198, 1040924135],\n",
    "        [1628816482, 2035717768],\n",
    "        [3143135738, 2669568898],\n",
    "        [2291874875,  615162511],\n",
    "        [1095314375, 2788700027],\n",
    "        [1823191131, 2195121142],\n",
    "        [2228445779, 1989120567],\n",
    "        [3885327141, 2474177994],\n",
    "        [2971148141, 1288397500],\n",
    "        [2027094719, 1391487009],\n",
    "        [2299672560, 2319724900],\n",
    "        [3074871096, 4195052087],\n",
    "        [3661317072, 1121612301],\n",
    "        [2983124547,  124694368],\n",
    "        [3665975848, 2288001433],\n",
    "        [2257298049,  758932273],\n",
    "        [1122978584, 3343290932],\n",
    "        [1878629911, 2726244928],\n",
    "        [2031137531, 2450789425],\n",
    "        [4090452839, 3843748098],\n",
    "        [ 210481107, 2877236582]\n",
    "    ]\n",
    "    \n",
    "    for m in range(2, 11):\n",
    "        # TF gradient will throw error if abs(n - m) >= 2, so constrain range of dimensions\n",
    "        min_n = max(m - 1, 2)\n",
    "        max_n = min(m + 2, 11)\n",
    "        for n in range(min_n, max_n):\n",
    "            assert(abs(m - n) < 2)\n",
    "            key = random.PRNGKeyArray(random.default_prng_impl(), jnp.array(keys[key_idx], dtype='uint32'))\n",
    "            args = check_full_vjp_args_(key, m, n)\n",
    "            assert(check_full_vjp_args_same_results(args[0]))\n",
    "            key_idx += 1\n",
    "            check_real_valued_full_vjp(*args)\n",
    "\n",
    "# NOTE(will) - TF calculates a different gradient for full SVD than Pytorch. \n",
    "# Algebraically, I can confirm the gradient used by TF is what is specified in\n",
    "# the reference by the differential formulas. It looks like Pytorch's gradient\n",
    "# does not incorporate the extra term for the full SVD case.\n",
    "check_real_valued_fulls_vjp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d769d84",
   "metadata": {},
   "source": [
    "## Complex Input SVD\n",
    "\n",
    "Reference: https://arxiv.org/pdf/1909.02659.pdf\n",
    "\n",
    "The reference documents the backward pass of the complex input partial SVD. Note that in Tensorflow, the reference formula is also used for the full case with the same additional term added. However, I'm relatively sure it's incorrect as it does not pass the numerical finite difference check that the partial case did. This [script](https://github.com/williamberman/SVD_autodiff/blob/will/svd-complex-full/svd_ad_jax.py) demonstrates.\n",
    "\n",
    "### Partial Backward SVD\n",
    "\n",
    "The derivation is mostly the same as the real case. All transposes are replaced by conjugate transposes, and we add a single additional term, $\\termc$.\n",
    "\n",
    "##### Partial SVD Gradient\n",
    "\n",
    "$\\gA = \\termu + \\terms + \\termv + \\termc$\n",
    "\n",
    "$\\termu = \\U [\\Ku + \\Kuh] \\S \\Vh$\n",
    "\n",
    "$\\terms = \\U (\\I_k \\circ \\gS ) \\Vh$\n",
    "\n",
    "$\\termv = \\U \\S [\\Kv + \\Kvh] \\Vh + \\U \\S^{-1} \\gVh (\\I_n - \\V \\Vh)$\n",
    "\n",
    "$\\termc = \\frac{1}{2} \\U \\S^{-1} ( \\Kch - \\Kc ) \\Vh $\n",
    "\n",
    "$\\Ku = \\F \\circ ( \\Uh \\gU ) $\n",
    "\n",
    "$\\Kv = \\F \\circ ( \\Vh \\gV ) $\n",
    "\n",
    "$\\Kc = \\I \\circ (\\Vh \\gV) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e6b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _H(x): return jnp.conj(x.T)\n",
    "\n",
    "def svd_vjp_complex_valued_partial(vals, grads):\n",
    "    U, S_vals, Vh = vals\n",
    "    gU, gS_vals, gVh = grads\n",
    "    \n",
    "    m = U.shape[0]\n",
    "    n = Vh.shape[1]\n",
    "    \n",
    "    if m > n:\n",
    "        gA, = svd_vjp_complex_valued_partial((_H(Vh), S_vals, _H(U)), (_H(gVh), gS_vals, _H(gU)))\n",
    "        return (_H(gA),)\n",
    "    \n",
    "    # m <= n\n",
    "    k = m\n",
    "    \n",
    "    gU = jnp.conj(gU)\n",
    "    gS_vals = jnp.conj(gS_vals)\n",
    "    gVh = jnp.conj(gVh)\n",
    "    \n",
    "    S = fill_diagonal(jnp.zeros((k, k)), S_vals)\n",
    "    gS = fill_diagonal(jnp.zeros((k, k)), gS_vals)\n",
    "    S_inv = jnp.linalg.inv(S)\n",
    "    \n",
    "    F_ = F(S_vals, m)\n",
    "    Fh = _H(F_)\n",
    "    \n",
    "    V = _H(Vh)\n",
    "    Uh = _H(U)\n",
    "    gV = _H(gVh)\n",
    "    \n",
    "    ############ term_U ##########\n",
    "    \n",
    "    K_u = F_ * (Uh @ gU)\n",
    "    K_uh = _H(K_u)\n",
    "    term_U = U @ (K_u + K_uh) @ S @ Vh\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ########## term_S ###########\n",
    "    \n",
    "    I = jnp.eye(k)\n",
    "    term_S = U @ (I * gS) @ Vh\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ######### term_V ############\n",
    "\n",
    "    K_v = F_ * (Vh @ gV)\n",
    "    K_vh = _H(K_v)\n",
    "    \n",
    "    I_n = jnp.eye(n)\n",
    "    \n",
    "    term_V1 = U @ S @ (K_v + K_vh) @ Vh\n",
    "    term_V2 = U @ S_inv @ gVh @ (I_n - V @ Vh)\n",
    "    \n",
    "    term_V = term_V1 + term_V2\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ########### term_C ###########\n",
    "    \n",
    "    K_c = jnp.eye(m) * (Vh @ gV)\n",
    "    K_ch = _H(K_c)\n",
    "    \n",
    "    term_C = 0.5 * U @ S_inv @ (K_ch - K_c) @ Vh\n",
    "    \n",
    "    ############################\n",
    "    \n",
    "    gA = jnp.conj(term_U + term_S + term_V + term_C)\n",
    "    \n",
    "    return (gA,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb906981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE only enabled for this test. Will make eariler tests\n",
    "# fail. I'm assuming it's some precision issue with jax\n",
    "# auto promoting to 64 bit datatypes that TF and torch\n",
    "# aren't using.\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "@jax.custom_vjp\n",
    "def svd_complex_valued_partial(A):\n",
    "    return jnp.linalg.svd(A, full_matrices=False, compute_uv=True)\n",
    "\n",
    "def svd_fwd_complex_valued_partial(A):\n",
    "    u, s, v = svd_complex_valued_partial(A)\n",
    "    return (u, s, v), (u, s, v)\n",
    "\n",
    "svd_complex_valued_partial.defvjp(svd_fwd_complex_valued_partial, svd_vjp_complex_valued_partial)\n",
    "\n",
    "def l(A):\n",
    "    u, s, v = svd_complex_valued_partial(A)\n",
    "    return jnp.real(u[0, -1] * v[-1, 0])\n",
    "\n",
    "# NOTE(will) - It's hard to get equal SVD factorizations for complex inputs between\n",
    "# jax and tf. Instead, check against numerical finite difference\n",
    "#\n",
    "# Adapted from:\n",
    "# https://github.com/Zhouquan-Wan/SVD_autodiff/blob/master/svd_ad_jax.py\n",
    "def check_complex_valued_partial_vjp(key, m, n):\n",
    "    k1, k2 = random.split(key)\n",
    "    \n",
    "    Ax = random.normal(k1, (m, n))\n",
    "    Ay = random.normal(k2, (m, n))\n",
    "    \n",
    "    A = jnp.array(Ax + 1.0j * Ay, dtype='complex128')\n",
    "    \n",
    "    # auto diff\n",
    "    lA, DA_ad = jax.value_and_grad(l)(A)\n",
    "    \n",
    "    # numerical\n",
    "    d = 1e-6\n",
    "    DA = jnp.zeros((m, n), dtype='complex128')\n",
    "    for i in range(0, m):\n",
    "        for j in range(0, n):\n",
    "            dA_real = jnp.zeros((m, n)).at[i, j].set(d)\n",
    "            dA_imag = jnp.zeros((m, n), dtype='complex128').at[i, j].set(d * 1.0j)\n",
    "            \n",
    "            real_approx = (l(A + dA_real) - lA) / d\n",
    "            imag_approx = (l(A + dA_imag) - lA) / d\n",
    "            \n",
    "            DA = DA.at[i, j].set(real_approx - 1.0j * imag_approx)\n",
    "    \n",
    "    assert_allclose(DA, DA_ad)\n",
    "    \n",
    "    return DA\n",
    "\n",
    "def check_complex_valued_partials_vjp():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(2, 11):\n",
    "        for n in range(2, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            check_complex_valued_partial_vjp(subkey, m, n)\n",
    "\n",
    "check_complex_valued_partials_vjp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72312621",
   "metadata": {},
   "source": [
    "#### Additional References:\n",
    "\n",
    "SVD real differentiability:\n",
    "- https://arxiv.org/pdf/1509.07838.pdf\n",
    "\n",
    "SVD complex differentiability:\n",
    "- https://giggleliu.github.io/2019/04/02/einsumbp.html\n",
    "\n",
    "General complex differentiability:\n",
    "- https://mediatum.ub.tum.de/doc/631019/631019.pdf\n",
    "\n",
    "Existing implementations:\n",
    "- [Jax](https://github.com/google/jax/blob/2a00533e3e686c1c9d7dfe9ed2a3b19217cfe76f/jax/_src/lax/linalg.py#L1578)\n",
    "- [Pytorch forward](https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3122)\n",
    "- [Pytorch backward](https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3228)\n",
    "- [Tensorflow backward](https://github.com/tensorflow/tensorflow/blob/bbe41abdcb2f7e923489bfa21cfb546b6022f330/tensorflow/python/ops/linalg_grad.py#L815)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
