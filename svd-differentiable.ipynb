{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b819698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-08 10:48:44.099720: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-09-08 10:48:44.120950: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import jax\n",
    "from jax import random\n",
    "import torch\n",
    "import torch.autograd.functional as TF\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "torch_jvp = TF.jvp\n",
    "torch_vjp = TF.vjp\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05bc7a9",
   "metadata": {},
   "source": [
    "## Cases\n",
    "$$\n",
    "\\newcommand{\\tr}{{\\mathrm{tr}}}\n",
    "\\newcommand{\\d}{{\\mathrm{d}}}\n",
    "\\newcommand{\\A}{{\\mathbf{A}}}\n",
    "\\newcommand{\\U}{{\\mathbf{U}}}\n",
    "\\renewcommand{\\S}{{\\mathbf{S}}}\n",
    "\\newcommand{\\V}{{\\mathbf{V}}}\n",
    "\\newcommand{\\F}{{\\mathbf{F}}}\n",
    "\\newcommand{\\I}{{\\mathbf{I}}}\n",
    "\\renewcommand{\\P}{{\\mathbf{P}}}\n",
    "\\newcommand{\\D}{{\\mathbf{D}}}\n",
    "\\newcommand{\\dA}{{\\d\\A}}\n",
    "\\newcommand{\\dU}{{\\d\\U}}\n",
    "\\newcommand{\\dS}{{\\d\\S}}\n",
    "\\newcommand{\\dV}{{\\d\\V}}\n",
    "\\newcommand{\\dP}{{\\d\\P}}\n",
    "\\newcommand{\\dD}{{\\d\\D}}\n",
    "\\newcommand{\\Ut}{{\\U^{\\top}}}\n",
    "\\newcommand{\\Vt}{{\\V^{\\top}}}\n",
    "\\newcommand{\\Vh}{{\\V^{H}}}\n",
    "\\newcommand{\\dAt}{{\\dA^{\\top}}}\n",
    "\\newcommand{\\dVt}{{\\dV^{\\top}}}\n",
    "\\newcommand{\\gA}{{\\overline{\\A}}}\n",
    "\\newcommand{\\gAt}{{\\gA^{\\top}}}\n",
    "\\newcommand{\\gU}{{\\overline{\\U}}}\n",
    "\\newcommand{\\gUt}{{\\gU^{\\top}}}\n",
    "\\newcommand{\\gS}{{\\overline{\\S}}}\n",
    "\\newcommand{\\gSt}{{\\gS^{\\top}}}\n",
    "\\newcommand{\\gV}{{\\overline{\\V}}}\n",
    "\\newcommand{\\gVt}{{\\gV^{\\top}}}\n",
    "\\newcommand{\\K}{{\\mathbf{K}}}\n",
    "\\newcommand{\\Ku}{{\\K_u}}\n",
    "\\newcommand{\\Kut}{{\\Ku^{\\top}}}\n",
    "\\newcommand{\\Vm}{{\\V_m}}\n",
    "\\newcommand{\\Vmt}{{\\Vm^{\\top}}}\n",
    "\\newcommand{\\Kv}{{\\K_v}}\n",
    "\\newcommand{\\Kvt}{{\\Kv^{\\top}}}\n",
    "\\newcommand{\\Vr}{{\\V_r}}\n",
    "\\newcommand{\\Vrt}{{\\Vr^{\\top}}}\n",
    "\\newcommand{\\Ft}{{\\F^{\\top}}}\n",
    "\\newcommand{\\gVm}{{\\gV_m}}\n",
    "\\newcommand{\\gVmt}{{\\gVm^{\\top}}}\n",
    "\\newcommand{\\gVr}{{\\gV_r}}\n",
    "\\newcommand{\\Kc}{{\\K_c}}\n",
    "\\newcommand{\\Kch}{{\\K_c^{\\dagger}}}\n",
    "\\newcommand{\\Vh}{{\\V^{\\dagger}}}\n",
    "\\newcommand{\\Uh}{{\\U^{\\dagger}}}\n",
    "\\newcommand{\\Kuh}{{\\Ku^{\\dagger}}}\n",
    "\\newcommand{\\Kvh}{{\\Kv^{\\dagger}}}\n",
    "\\newcommand{\\Vmh}{{\\Vm^{\\dagger}}}\n",
    "\\newcommand{\\gVmh}{{\\gVm^{\\dagger}}}\n",
    "\\newcommand{\\Vrh}{{\\Vr^{\\dagger}}}\n",
    "\\newcommand{\\gVh}{{\\gV^{\\dagger}}}\n",
    "\\newcommand{\\termu}{{\\mathrm{term}_U}}\n",
    "\\newcommand{\\terms}{{\\mathrm{term}_S}}\n",
    "\\newcommand{\\termv}{{\\mathrm{term}_V}}\n",
    "\\newcommand{\\termc}{{\\mathrm{term}_C}}\n",
    "$$\n",
    "The derivative of the SVD operation is determined by\n",
    "\n",
    "1. Computing the full SVD vs the \"thin\"/\"partial\" SVD\n",
    "2. complex vs real inputs\n",
    "3. Computing the complete factorization $\\U\\S\\Vh$ vs computing only the singular values $\\S$\n",
    "\n",
    "Computing only the singular values does not change the differential or gradient formulas and can be considered a side case of the full factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac90ffc",
   "metadata": {},
   "source": [
    "## Real valued partial SVD\n",
    "\n",
    "Reference: https://j-towns.github.io/papers/svd-derivative.pdf\n",
    "\n",
    "### Forward mode\n",
    "\n",
    "The differential formulas $\\dU$, $\\dS$, and $\\dV$ in terms of $\\dA$, $\\U$, $\\S$, and $\\V$ are found from the chain rule.\n",
    "\n",
    "##### Chain rule\n",
    "\n",
    "$\\dA = \\dU \\S \\Vt + \\U \\dS \\Vt + \\U \\S \\dVt$\n",
    "\n",
    "TODO(will) - Is this really the chain rule? Deriving this is a little funky because you end up taking partial derivatives with respect to matrices which I'm not sure how to define. The rule for expressing the differential and the product is in 2.1/2.2 https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "\n",
    "##### Differential formulas\n",
    "\n",
    "$\\dU = \\U ( \\F \\circ [\\Ut \\dA \\V \\S + \\S \\Vt \\dAt \\U] ) + (\\I_m - \\U \\Ut ) \\dA \\V \\S^{-1}$\n",
    "\n",
    "$\\dS = \\I_k \\circ [\\Ut \\dA \\V]$\n",
    "\n",
    "$\\dV = \\V (\\F \\circ [\\S \\Ut \\dA \\V + \\Vt \\dAt \\U \\S]) + (\\I_n - \\V \\Vt) \\dAt \\U \\S^{-1}$\n",
    "\n",
    "where\n",
    "\n",
    "$F_{ij} = \\frac{1}{s_j^2 - s_i^2}, i \\neq j$\n",
    "\n",
    "$F_{ij} = 0$ otherwise\n",
    "\n",
    "$k$ is the rank of $\\A$\n",
    "\n",
    "$\\U$ is $m \\times k$\n",
    "\n",
    "$\\S$ is $k \\times k$\n",
    "\n",
    "$\\Vt$ is $k \\times n$\n",
    "\n",
    "##### Numerical instability\n",
    "Both $\\S^{-1}$ and $\\F$ cause numerical instability in $\\dU$ and $\\dV$.  \n",
    "\n",
    "$\\S^{-1}$ causes numerical instability with zero or very small singular values. The inverse of a diagonal matrix is the element wise reciprocals of the inverting matrix's diagonal. Zero singular values will cause $\\S^{-1}$ to have infinite elements. Very small singular values will cause $\\S^{-1}$ to have very large floating point elements.\n",
    "\n",
    "$\\F$ divides by the difference of all pairs of singular values. Repeated singular values will cause infinite elements of $\\F$. Close singular values will cause very large floating point elements of $\\F$.  \n",
    "\n",
    "##### Computing only singular values\n",
    "Only compute the differential for $\\S$. $\\dU$ and $\\dV$ are ignored. Because $\\dU$ and $\\dV$ are not computed, $\\F$ and $\\S^{-1}$ are not needed, so there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56584cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_partial(A, dA):    \n",
    "    U, S_vals, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=False)\n",
    "\n",
    "    S = jnp.diag(S_vals)\n",
    "    Ut = U.T\n",
    "    V = Vt.T\n",
    "    dAt = dA.T\n",
    "\n",
    "    k = S.shape[0]\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "\n",
    "    I_k = jnp.eye(k)\n",
    "    I_m = jnp.eye(m)\n",
    "    I_n = jnp.eye(n)\n",
    "\n",
    "    S_inv = jnp.linalg.inv(S)\n",
    "\n",
    "    F_ = F(S_vals, k)\n",
    "\n",
    "    dU = U @ (F_ * ((Ut @ dA @ V @ S) + (S @ Vt @ dAt @ U))) + ((I_m - U @ Ut) @ dA @ V @ S_inv)\n",
    "    \n",
    "    # Note that the `I_k *` is extraneous. It zeros out the rest of the matrix besides the diagonal.\n",
    "    # We only return `dS_vals` which takes only the diagonal of `dS` anyway. \n",
    "    dS = I_k * (Ut @ dA @ V)\n",
    "    dS_vals = jnp.diagonal(dS)\n",
    "    \n",
    "    dV = V @ (F_ * (S @ Ut @ dA @ V + Vt @ dAt @ U @ S)) + (I_n - V @ Vt) @ dAt @ U @ S_inv\n",
    "    \n",
    "    return (U, S_vals, Vt), (dU, dS_vals, dV.T)\n",
    "\n",
    "def F(S_vals, k):\n",
    "    F_i_j = lambda i, j: lax.cond(i == j, lambda: 0., lambda: 1 / (S_vals[j]**2 - S_vals[i]**2))\n",
    "    F_fun = jax.vmap(jax.vmap(F_i_j, (None, 0)), (0, None))\n",
    "\n",
    "    indices = jnp.arange(k)\n",
    "    F_ = F_fun(indices, indices)\n",
    "    \n",
    "    return F_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b00e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_svd_uv(a, b):\n",
    "    (U, S, Vt), (dU, dS, dVt) = a\n",
    "    (U_, S_, Vt_), (dU_, dS_, dVt_) = b\n",
    "    \n",
    "    assert_allclose(U, U_)\n",
    "    assert_allclose(S, S_)\n",
    "    assert_allclose(Vt, Vt_)\n",
    "    assert_allclose(dU, dU_)\n",
    "    assert_allclose(dS, dS_)\n",
    "    assert_allclose(dVt, dVt_)\n",
    "\n",
    "def check_allclose(l, r):\n",
    "    return jnp.allclose(jnp.array(l), jnp.array(r), atol=1e-04)\n",
    "\n",
    "def assert_allclose(l, r):\n",
    "    assert(check_allclose(l, r))\n",
    "        \n",
    "def jax_real_valued_partial(A, dA):\n",
    "    A = jnp.array(A)\n",
    "    dA = jnp.array(dA)\n",
    "    \n",
    "    return jax.jvp(\n",
    "        lambda A: jnp.linalg.svd(A, compute_uv=True, full_matrices=False), \n",
    "        (A,), \n",
    "        (dA,)\n",
    "    )\n",
    "\n",
    "def pytorch_real_valued_partial(A, dA):\n",
    "    A = torch.tensor(A)\n",
    "    dA = torch.tensor(dA)\n",
    "    \n",
    "    return torch_jvp(lambda A: torch.linalg.svd(A, full_matrices=False), A, dA)\n",
    "    \n",
    "def check_real_valued_partial(A, dA):\n",
    "    jax_res = jax_real_valued_partial(A, dA)\n",
    "    torch_res = pytorch_real_valued_partial(A, dA)\n",
    "    res = svd_jvp_real_valued_partial(jnp.array(A), jnp.array(dA))\n",
    "    \n",
    "    # example == jax == torch\n",
    "    assert_svd_uv(jax_res, torch_res)\n",
    "    assert_svd_uv(jax_res, res)\n",
    "    \n",
    "    return jax_res, torch_res, res\n",
    "\n",
    "def check_real_valued_partial_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "    dA = jnp.ones_like(A)\n",
    "            \n",
    "    A = A.tolist()\n",
    "    dA = dA.tolist()\n",
    "    \n",
    "    return A, dA\n",
    "  \n",
    "def check_real_valued_partials():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_partial_args(subkey, m, n)\n",
    "            check_real_valued_partial(*args)\n",
    "\n",
    "check_real_valued_partials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc7a36",
   "metadata": {},
   "source": [
    "### Reverse mode\n",
    "\n",
    "##### Chain rule\n",
    "$ \\tr(\\gAt \\dA) = \\tr(\\gUt \\dU) + \\tr(\\gSt \\dS) + \\tr(\\gVt \\dV) $\n",
    "\n",
    "##### Gradient formula\n",
    "\n",
    "The formula for A's gradient has terms $\\termu$, $\\terms$, and $\\termv$ found from the respective trace terms in the chain rule\n",
    "\n",
    "$\\gA = \\termu + \\terms + \\termv$\n",
    "\n",
    "$\\termu = \\U [\\Ku + \\Kut] \\S \\Vt + [ \\I_m - \\U \\Ut ] \\gU \\S^{-1} \\Vt$\n",
    "\n",
    "$\\terms = \\U (\\I_k \\circ \\gS ) \\Vt$\n",
    "\n",
    "$\\termv = \\U \\S [\\Kv + \\Kvt] \\Vt + \\U \\S^{-1} \\gVt (\\I_n - \\V \\Vt)$\n",
    "\n",
    "$\\Ku = \\F \\circ ( \\Ut \\gU ) $\n",
    "\n",
    "$\\Kv = \\F \\circ ( \\Vt \\gV ) $\n",
    "\n",
    "Note that the second terms in $\\termu$ and $\\termv$ are to account for when $\\A$ is non-square. They will both be $0$ when $A$ is square due to $\\U \\Ut$ and $\\V \\Vt$ both being equal to identity matrices because $\\U$ and $\\V$ are individually composed of orthogonal vectors. For a non-square $A$, one of the two terms will be $0$ corresponding to which of $\\U$ or $\\V$ is rank-deficient. Without loss of generality, we can omit one of the two second terms. I.e. for a $m \\times n$ input, only compute the gradient for matrices where $m \\lt n$ omitting the second term on $\\termu$ because only $\\V$ will ever be rank deficient. For matrices where $m \\gt n$, we can compute the gradient of the transpose and then transpose the resulting gradient.   \n",
    "\n",
    "##### Numerical instability\n",
    "$\\S^{-1}$ and $\\F$ cause numerical instability in $\\gA$ through $\\termu$ and $\\termv$. The explanation is the same as in the forward mode.\n",
    "\n",
    "##### Computing only singular values\n",
    "When only computing singular values, no $\\U$ or $\\Vt$ were produced to impact a resulting objective function. $\\gU$ and $\\gVt$ must then be zero, and so are $\\termu$ and $\\termv$. This means that $\\gA = \\terms$, and there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32dbfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_partial(A, U, S_vals, Vt, gU, gS_vals, gVt):\n",
    "    S = jnp.diag(S_vals)\n",
    "    gS = jnp.diag(gS_vals)\n",
    "    \n",
    "    k = S.shape[0]\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    I_m = jnp.eye(m)\n",
    "    I_k = jnp.eye(k)\n",
    "    I_n = jnp.eye(n)\n",
    "    \n",
    "    V = Vt.T\n",
    "    Ut = U.T\n",
    "    gUt = gU.T\n",
    "    gV = gVt.T\n",
    "    \n",
    "    S_inv = jnp.linalg.inv(S)\n",
    "    \n",
    "    F_ = F(S_vals, k)\n",
    "    \n",
    "    ############### term_U ################\n",
    "    \n",
    "    K_u = F_ * (Ut @ gU)\n",
    "    K_ut = K_u.T\n",
    "    \n",
    "    term_U = U @ (K_u + K_ut) @ S @ Vt + (I_m - U @ Ut) @ gU @ S_inv @ Vt\n",
    "    \n",
    "    ########################################\n",
    "    \n",
    "    ############## term_S ################\n",
    "    \n",
    "    term_S = U @ (I_k * gS) @ Vt\n",
    "    \n",
    "    #####################################\n",
    "    \n",
    "    ############# term_V ################\n",
    "    \n",
    "    K_v = F_ * (Vt @ gV)\n",
    "    K_vt = K_v.T\n",
    "    \n",
    "    term_V = U @ S @ (K_v + K_vt) @ Vt + U @ S_inv @ gVt @ (I_n - V @ Vt)\n",
    "    \n",
    "    ####################################\n",
    "    \n",
    "    gA = term_U + term_S + term_V\n",
    "    \n",
    "    return gA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08d7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_partial_(A, gU, gS, gVt):\n",
    "    A = jnp.array(A)\n",
    "    gU = jnp.array(gU)\n",
    "    gS = jnp.array(gS)\n",
    "    gVt = jnp.array(gVt)\n",
    "    \n",
    "    U, S, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=False)\n",
    "    \n",
    "    gA = svd_vjp_real_valued_partial(A, U, S, Vt, gU, gS, gVt)\n",
    "    \n",
    "    return gA\n",
    "\n",
    "def jax_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    A = jnp.array(A)\n",
    "    gU = jnp.array(gU)\n",
    "    gS = jnp.array(gS)\n",
    "    gVt = jnp.array(gVt)    \n",
    "    \n",
    "    _, vjp_fun = jax.vjp(\n",
    "        lambda A: jnp.linalg.svd(A, compute_uv=True, full_matrices=False), \n",
    "        A,\n",
    "    )\n",
    "    \n",
    "    gA, = vjp_fun((gU, gS, gVt))\n",
    "    \n",
    "    return gA\n",
    "\n",
    "def pytorch_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    A = torch.tensor(A)\n",
    "    gU = torch.tensor(gU)\n",
    "    gS = torch.tensor(gS)\n",
    "    gVt = torch.tensor(gVt)\n",
    "    \n",
    "    _, gA = torch_vjp(lambda A: torch.linalg.svd(A, full_matrices=False), A, (gU, gS, gVt))\n",
    "    \n",
    "    return gA\n",
    "    \n",
    "def check_real_valued_partial_vjp(A, gU, gS, gVt):\n",
    "    jax_gA = jax_real_valued_partial_vjp(A, gU, gS, gVt)\n",
    "    torch_gA = pytorch_real_valued_partial_vjp(A, gU, gS, gVt)\n",
    "    res_gA = svd_vjp_real_valued_partial_(A, gU, gS, gVt)\n",
    "    \n",
    "    # example == jax == torch\n",
    "    assert_allclose(jax_gA, torch_gA)\n",
    "    assert_allclose(jax_gA, res_gA)\n",
    "    \n",
    "    return jax_gA, torch_gA, res_gA\n",
    "    \n",
    "def check_real_valued_partial_vjp_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "    \n",
    "    k = min(m, n)\n",
    "\n",
    "    gU = jnp.ones((m, k))\n",
    "    gS = jnp.ones(k)\n",
    "    gVt = jnp.ones((k, n))\n",
    "    \n",
    "    A = A.tolist()\n",
    "    gU = gU.tolist()\n",
    "    gS = gS.tolist()\n",
    "    gVt = gVt.tolist()\n",
    "    \n",
    "    return A, gU, gS, gVt\n",
    "\n",
    "def check_real_valued_partials_vjp():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_partial_vjp_args(subkey, m, n)\n",
    "            check_real_valued_partial_vjp(*args)\n",
    "\n",
    "            \n",
    "check_real_valued_partials_vjp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b24fa",
   "metadata": {},
   "source": [
    "## Real valued full SVD\n",
    "\n",
    "Reference: https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "\n",
    "Without loss of generality, assume rectangular inputs have more columns than rows. For inputs with less columns than rows, compute gradients and differentials for the transpose of the input and then transpose the results.  \n",
    "\n",
    "\n",
    "### Forward mode\n",
    "\n",
    "The differential formulas are found by a similar process to the partial case.\n",
    "\n",
    "##### Differential formulas\n",
    "\n",
    "$ \\dU = \\U [\\F \\circ ( \\dP_1 \\S_1 + \\S_1 \\dP_1^{\\top} ) ] $\n",
    "\n",
    "$ \\Ut \\dA \\V = \\begin{bmatrix} \\dP_1 & \\dP_2 \\end{bmatrix} $\n",
    "\n",
    "$ \\S = \\begin{bmatrix} \\S_1 & 0 \\end{bmatrix} $\n",
    "\n",
    "$ \\dP_1 $ is $m \\times m$\n",
    "\n",
    "$ \\S_1 $ is $m \\times m$\n",
    "\n",
    "---\n",
    "\n",
    "$ \\dS = \\I \\circ [\\U^{\\top} \\dA \\V ] $\n",
    "\n",
    "---\n",
    "\n",
    "$ \\dV = \\V \\dD $\n",
    "\n",
    "$ \\dD = \\begin{bmatrix} \\dD_1 & -\\dD_2 \\\\ \\dD_2^{\\top} & \\dD_3 \\end{bmatrix} $\n",
    "\n",
    "$ \\dD_1 = \\F \\circ (\\S_1 \\dP_1 + \\dP_1^{\\top} \\S_1 ) $\n",
    "\n",
    "$ \\dD_2 = \\S_1^{-1} \\dP_2 $\n",
    "\n",
    "$ \\dD_3 = \\mathbf{0} $\n",
    "\n",
    "$ \\dD $ is $ n \\times n $\n",
    "\n",
    "$ \\dD_1 $ is $ m \\times m $\n",
    "\n",
    "$ \\dD_2 $ is $ m \\times (n - m) $\n",
    "\n",
    "$ \\dD_3 $ is $ (n - m) \\times (n - m) $\n",
    "\n",
    "##### Numerical instability\n",
    "The full SVD has the same numerical instability as the partial SVD.\n",
    "\n",
    "Both $\\S_1^{-1}$ and $\\F$ cause numerical instability in $\\dU$ and $\\dV$.\n",
    "\n",
    "Additionally, we know ahead of time $\\F$ will contain divisions by zero when $|n - m| \\geq 2$ because this guarantees at least two zero valued singular values. \n",
    "\n",
    "##### Computing only singular values\n",
    "Same as in the partial SVD. Only computing the singular values ignores $\\dU$ and $\\dV$ which means there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91523b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_full(A, dA):\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    if m > n:\n",
    "        (U, S, Vt), (dU, dS, dVt) = svd_jvp_real_valued_full(A.T, dA.T)\n",
    "        return (Vt.T, S, U.T), (dVt.T, dS, dU.T)\n",
    "    \n",
    "    # m <= n\n",
    "    \n",
    "    U, S_vals, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=True)\n",
    "        \n",
    "    Ut = U.T\n",
    "    V = Vt.T\n",
    "    \n",
    "    I = fill_diagonal(jnp.zeros((m, n)), 1)\n",
    "\n",
    "    # Can directly create S_1 without creating S which has added zero columns\n",
    "    # There will be m singular values, so S_1 will be m x m.\n",
    "    S_1 = jnp.diag(S_vals)\n",
    "    S_1_inv = jnp.linalg.inv(S_1)\n",
    "    \n",
    "    # m is number singular values insted of k as in partial version\n",
    "    F_ = F(S_vals, m)\n",
    "    \n",
    "    ########## dU ###########\n",
    "\n",
    "    dP = Ut @ dA @ V\n",
    "    dP_1 = dP[:, :m]\n",
    "    dP_2 = dP[:, m:]\n",
    "    dP_1t = dP_1.T\n",
    "    \n",
    "    dU = U @ (F_ * (dP_1 @ S_1 + S_1 @ dP_1t))\n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ######### dS ##########\n",
    "    \n",
    "    dS = I * (Ut @ dA @ V)\n",
    "    dS_vals = jnp.diagonal(dS)\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    ######## dV ##########\n",
    "    \n",
    "    dD_1 = F_ * (S_1 @ dP_1 + dP_1t @ S_1)\n",
    "    dD_2 = S_1_inv @ dP_2\n",
    "    dD_3 = jnp.zeros((n-m, n-m))\n",
    "    neg_dD_2 = -dD_2\n",
    "    dD_2t = dD_2.T\n",
    "    \n",
    "    dD_top = jnp.concatenate((dD_1, neg_dD_2), axis=1)\n",
    "    dD_bottom = jnp.concatenate((dD_2t, dD_3), axis=1)\n",
    "    \n",
    "    dD = jnp.concatenate((dD_top, dD_bottom), axis=0)\n",
    "    \n",
    "    dV = V @ dD\n",
    "    \n",
    "    #####################\n",
    "    \n",
    "    return (U, S_vals, Vt), (dU, dS_vals, dV.T)\n",
    "\n",
    "# modified version of fill_diagonal because jax does not implement it.\n",
    "# https://github.com/numpy/numpy/blob/v1.23.0/numpy/lib/index_tricks.py#L779-L910\n",
    "def fill_diagonal(a, val, start=None):\n",
    "    m, n = a.shape\n",
    "    \n",
    "    end = None\n",
    "    step = a.shape[1] + 1\n",
    "    \n",
    "    if start:\n",
    "        start_row, start_col = start\n",
    "        start = start_row * n + start_col\n",
    "        a = a.flatten().at[start:end:step].set(val)\n",
    "    else:\n",
    "        a = a.flatten().at[:end:step].set(val)\n",
    "        \n",
    "    a = a.reshape((m, n))\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df01bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_jvp_real_valued_full_(A, dA):\n",
    "    A = jnp.array(A)\n",
    "    dA = jnp.array(dA)\n",
    "    \n",
    "    return svd_jvp_real_valued_full(A, dA)\n",
    "    \n",
    "def pytorch_real_valued_full(A, dA):\n",
    "    A = torch.tensor(A)\n",
    "    dA = torch.tensor(dA)\n",
    "    \n",
    "    return torch_jvp(lambda A: torch.linalg.svd(A, full_matrices=True), A, dA)\n",
    "\n",
    "def check_real_valued_full(A, dA):\n",
    "    # NOTE - JAX does not implement full\n",
    "    torch_res = pytorch_real_valued_partial(A, dA)\n",
    "    res = svd_jvp_real_valued_partial(jnp.array(A), jnp.array(dA))\n",
    "    \n",
    "    assert_svd_uv(torch_res, res)\n",
    "    \n",
    "    return torch_res, res\n",
    "\n",
    "def check_real_valued_full_args(key, m, n):\n",
    "    A = random.normal(key, (m, n))\n",
    "    dA = jnp.ones_like(A)\n",
    "            \n",
    "    A = A.tolist()\n",
    "    dA = dA.tolist()\n",
    "    \n",
    "    return A, dA\n",
    "  \n",
    "def check_real_valued_fulls():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(3, 11):\n",
    "        for n in range(3, 11):\n",
    "            key, subkey = random.split(key)\n",
    "            args = check_real_valued_full_args(subkey, m, n)\n",
    "            check_real_valued_full(*args)\n",
    "\n",
    "check_real_valued_fulls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be40654",
   "metadata": {},
   "source": [
    "### Backward mode\n",
    "\n",
    "The backward mode for the full case is found by the same process as in the partial case, but the complete formula is not documented in the reference. \n",
    "\n",
    "##### Gradient formula\n",
    "\n",
    "$\\gA = \\termu + \\terms + \\termv$\n",
    "\n",
    "$\\termu = \\U [\\Ku + \\Kut] \\S_1 \\Vmt $\n",
    "\n",
    "$\\terms = \\U (\\I \\circ \\gS) \\Vt$\n",
    "\n",
    "$\\termv = \\U \\S_1 [\\Kv + \\Kvt] \\Vmt + \\U \\S_1^{-1} [ \\gVmt \\Vr - \\Vmt \\gVr ] \\Vrt$\n",
    "\n",
    "$\\Ku = \\F \\circ (\\Ut\\gU)$\n",
    "\n",
    "$\\Kv = \\F \\circ (\\Vmt \\gVm)$\n",
    "\n",
    "$ \\Vm $ is the left most $m$ columns of $\\V $.\n",
    "\n",
    "$ \\gVm $ is the left most $m$ columns of $\\gV $.\n",
    "\n",
    "$ \\Vr $ is the right most $n - m$ columns of $\\V$.\n",
    "\n",
    "$ \\gVr $ is the right most $n - m$ columns of $\\gV$.\n",
    "\n",
    "##### Alternate $\\termv$\n",
    "\n",
    "Using the fact that $\\I = \\Vr\\Vrt + \\Vm\\Vmt$, we can massage $\\termv$ so that it is equal to $\\termv$ from the partial SVD backward pass with one additional term.\n",
    "\n",
    "$\\termv = \\U \\S_1 [\\Kv + \\Kvt] \\Vmt + \\U \\S_1^{-1} \\gVmt [ \\I - \\Vm \\Vmt ] - \\U \\S_1^{-1} \\Vmt \\gVr \\Vrt $\n",
    "\n",
    "##### Numerical instability\n",
    "$\\S^{-1}$ and $\\F$ cause numerical instability in $\\gA$ through $\\termu$ and $\\termv$. The explanation is the same as in the forward mode.\n",
    "\n",
    "##### Computing only singular values\n",
    "When only computing singular values, no $\\U$ or $\\Vt$ were produced to impact a resulting objective function. $\\gU$ and $\\gVt$ must then be zero, and so are $\\termu$ and $\\termv$. This means that $\\gA = \\terms$, and there is no numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1610d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_full(A, U, S_vals, Vt, gU, gS_vals, gVt):\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    if m > n:\n",
    "        gA = svd_vjp_real_valued_full(A.T, Vt.T, S_vals, U.T, gVt.T, gS_vals, gU.T)\n",
    "        return gA.T\n",
    "    \n",
    "    # m <= n\n",
    "    \n",
    "    S = fill_diagonal(jnp.zeros((m, n)), S_vals)\n",
    "    gS = fill_diagonal(jnp.zeros((m, n)), gS_vals)\n",
    "    S_1 = fill_diagonal(jnp.zeros((m, m)), S_vals)\n",
    "    S_1t = S_1.T\n",
    "    S_1_inv = jnp.linalg.inv(S_1)\n",
    "    \n",
    "    F_ = F(S_vals, m)\n",
    "    Ft = F_.T\n",
    "    \n",
    "    V = Vt.T\n",
    "    Ut = U.T\n",
    "    gSt = gS.T\n",
    "    gV = gVt.T\n",
    "    \n",
    "    Vm = V[:,:m]\n",
    "    Vr = V[:,m:]\n",
    "    Vmt = Vm.T\n",
    "    Vrt = Vr.T\n",
    "    \n",
    "    gVm = gV[:,:m]\n",
    "    gVr = gV[:,m:]\n",
    "    gVmt = gVm.T\n",
    "    \n",
    "    \n",
    "    ############ term_U ##########\n",
    "    \n",
    "    K_u = F_ * (Ut @ gU)\n",
    "    K_ut = K_u.T\n",
    "    term_U = U @ (K_u + K_ut) @ S_1 @ Vmt\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ########## term_S ###########\n",
    "    \n",
    "    I = fill_diagonal(jnp.zeros((m, n)), 1)\n",
    "    term_S = U @ (I * gS) @ Vt\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ######### term_V ############\n",
    "\n",
    "    K_v = F_ * (Vmt @ gVm)\n",
    "    K_vt = K_v.T\n",
    "    \n",
    "    term_V1 = U @ S_1t @ (K_v + K_vt) @ Vmt\n",
    "    term_V2 = U @ S_1_inv @ (gVmt @ Vr - Vmt @ gVr) @ Vrt\n",
    "    \n",
    "    term_V = term_V1 + term_V2\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    gA = term_U + term_S + term_V\n",
    "    \n",
    "    return gA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7cea8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vjp_real_valued_full_(A, gU, gS, gVt):\n",
    "    A = jnp.array(A)\n",
    "    gU = jnp.array(gU)\n",
    "    gS = jnp.array(gS)\n",
    "    gVt = jnp.array(gVt)\n",
    "    \n",
    "    U, S, Vt = jnp.linalg.svd(A, compute_uv=True, full_matrices=True)\n",
    "    \n",
    "    gA = svd_vjp_real_valued_full(A, U, S, Vt, gU, gS, gVt)\n",
    "    \n",
    "    return gA\n",
    "    \n",
    "def tf_full_vjp(A, gU, gS, gVt):\n",
    "    A = tf.constant(A)\n",
    "    gU = tf.constant(gU)\n",
    "    gS = tf.constant(gS)\n",
    "    gVt = tf.constant(gVt)\n",
    "    \n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(A)\n",
    "        S, U, V = tf.linalg.svd(A, full_matrices=True, compute_uv=True)\n",
    "        Vt = tf.transpose(V)\n",
    "        \n",
    "    gA = g.gradient((S, U, Vt), A, (gS, gU, gVt))\n",
    "        \n",
    "    return gA\n",
    "\n",
    "def check_real_valued_full_vjp(A, gU, gS, gVt):\n",
    "    tf_gA = tf_full_vjp(A, gU, gS, gVt)\n",
    "    res_gA = svd_vjp_real_valued_full_(A, gU, gS, gVt)\n",
    "    \n",
    "    assert_allclose(tf_gA, res_gA)\n",
    "    \n",
    "    return tf_gA, res_gA\n",
    "\n",
    "def check_full_vjp_args(key, m, n, dtype=None):\n",
    "    # Keep trying until we get an A where the factorizations are the same\n",
    "    while True:\n",
    "        key, subkey = random.split(key)\n",
    "        A, gU, gS, gVt = check_full_vjp_args_(subkey, m, n, dtype=dtype)\n",
    "        \n",
    "        if check_full_vjp_args_same_results(A):\n",
    "            break\n",
    "\n",
    "    return key, (A, gU, gS, gVt)\n",
    "\n",
    "# NOTE(will) - tf and jax do not always give the same U and V results. \n",
    "# When U and V differ, they differ only by sign at particular points but are\n",
    "# still both valid factorizations\n",
    "def check_full_vjp_args_same_results(A):\n",
    "    tf_S, tf_U, tf_V = tf.linalg.svd(tf.constant(A), full_matrices=True, compute_uv=True)\n",
    "    tf_Vt = tf.transpose(tf_V, conjugate=True)\n",
    "        \n",
    "    jax_U, jax_S, jax_Vt = jnp.linalg.svd(jnp.array(A), full_matrices=True, compute_uv=True)\n",
    "    \n",
    "    return check_allclose(jax_U, tf_U) and check_allclose(jax_S, tf_S) and check_allclose(jax_Vt, tf_Vt)\n",
    "\n",
    "def check_full_vjp_args_(key, m, n, dtype=None):\n",
    "    A = random.normal(key, (m, n), dtype=dtype)\n",
    "\n",
    "    k = min(m, n)\n",
    "\n",
    "    gU = jnp.zeros((m, m), dtype=dtype)\n",
    "    gS = jnp.zeros(k)\n",
    "    gVt = jnp.ones((n, n), dtype=dtype)\n",
    "\n",
    "    A = A.tolist()\n",
    "    gU = gU.tolist()\n",
    "    gS = gS.tolist()\n",
    "    gVt = gVt.tolist()\n",
    "\n",
    "    return A, gU, gS, gVt\n",
    "  \n",
    "def check_real_valued_fulls_vjp():\n",
    "    key_idx = 0\n",
    "    \n",
    "    keys = [\n",
    "        [1278412471, 2182328957],\n",
    "        [2205739499, 3850766070],\n",
    "        [3971862319, 2771547813],\n",
    "        [2936998373,  532740107],\n",
    "        [  41787198, 1040924135],\n",
    "        [1628816482, 2035717768],\n",
    "        [3143135738, 2669568898],\n",
    "        [2291874875,  615162511],\n",
    "        [1095314375, 2788700027],\n",
    "        [1823191131, 2195121142],\n",
    "        [2228445779, 1989120567],\n",
    "        [3885327141, 2474177994],\n",
    "        [2971148141, 1288397500],\n",
    "        [2027094719, 1391487009],\n",
    "        [2299672560, 2319724900],\n",
    "        [3074871096, 4195052087],\n",
    "        [3661317072, 1121612301],\n",
    "        [2983124547,  124694368],\n",
    "        [3665975848, 2288001433],\n",
    "        [2257298049,  758932273],\n",
    "        [1122978584, 3343290932],\n",
    "        [1878629911, 2726244928],\n",
    "        [2031137531, 2450789425],\n",
    "        [4090452839, 3843748098],\n",
    "        [ 210481107, 2877236582]\n",
    "    ]\n",
    "    \n",
    "    for m in range(2, 11):\n",
    "        # TF gradient will throw error if abs(n - m) >= 2, so constrain range of dimensions\n",
    "        min_n = max(m - 1, 2)\n",
    "        max_n = min(m + 2, 11)\n",
    "        for n in range(min_n, max_n):\n",
    "            assert(abs(m - n) < 2)\n",
    "            key = random.PRNGKeyArray(random.default_prng_impl(), jnp.array(keys[key_idx], dtype='uint32'))\n",
    "            args = check_full_vjp_args_(key, m, n)\n",
    "            assert(check_full_vjp_args_same_results(args[0]))\n",
    "            key_idx += 1\n",
    "            check_real_valued_full_vjp(*args)\n",
    "\n",
    "# NOTE(will) - TF calculates a different gradient for full SVD than Pytorch. \n",
    "# Algebraically, I can confirm the gradient used by TF is what is specified in\n",
    "# the reference by the differential formulas. It looks like Pytorch's gradient\n",
    "# does not incorporate the extra term for the full SVD case.\n",
    "check_real_valued_fulls_vjp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da987e1",
   "metadata": {},
   "source": [
    "## Complex valued SVD\n",
    "\n",
    "Reference: https://arxiv.org/pdf/1909.02659.pdf\n",
    "\n",
    "### Backward mode\n",
    "\n",
    "In the complex case, all transposes are replaced by conjugate transposes, and a single additional term is added for both the full and partial SVDs. \n",
    "\n",
    "Note that the reference does not explicitly say that the additional term can be added for both the partial and the full cases. The additional term is found in terms of a square input and noting that it holds as well for non-square input. In the reference, the additional non-square terms are for the partial SVD, but the tensorflow implementation includes the complex term in the full SVD case.\n",
    "\n",
    "##### Additional term\n",
    "\n",
    "$ \\frac{1}{2} \\U \\S^{-1} ( \\Kch - \\Kc ) \\Vh $\n",
    "\n",
    "$ \\Kc = \\I \\circ (\\Vh \\gV) $\n",
    "\n",
    "##### Partial SVD Gradient\n",
    "\n",
    "$\\gA = \\termu + \\terms + \\termv + \\termc$\n",
    "\n",
    "$\\termu = \\U [\\Ku + \\Kuh] \\S \\Vh$\n",
    "\n",
    "$\\terms = \\U (\\I_k \\circ \\gS ) \\Vh$\n",
    "\n",
    "$\\termv = \\U \\S [\\Kv + \\Kvh] \\Vh + \\U \\S^{-1} \\gVh (\\I_n - \\V \\Vh)$\n",
    "\n",
    "$\\termc = \\frac{1}{2} \\U \\S^{-1} ( \\Kch - \\Kc ) \\Vh $\n",
    "\n",
    "$\\Ku = \\F \\circ ( \\Uh \\gU ) $\n",
    "\n",
    "$\\Kv = \\F \\circ ( \\Vh \\gV ) $\n",
    "\n",
    "##### Full SVD Gradient\n",
    "\n",
    "$\\gA = \\termu + \\terms + \\termv + \\termc$\n",
    "\n",
    "$\\termu = \\U [\\Ku + \\Kuh] \\S_1 \\Vmh $\n",
    "\n",
    "$\\terms = \\U (\\I \\circ \\gS) \\Vh$\n",
    "\n",
    "$\\termv = \\U \\S_1 [\\Kv + \\Kvh] \\Vmh + \\U \\S_1^{-1} \\gVmh (\\I_n - \\Vm \\Vmh) - \\U \\S_1^{-1} \\Vmh \\gVr \\Vrh$\n",
    "\n",
    "$\\termc = \\frac{1}{2} \\U \\S^{-1} ( \\Kch - \\Kc ) \\Vh $\n",
    "\n",
    "$\\Ku = \\F \\circ (\\Uh\\gU)$\n",
    "\n",
    "$\\Kv = \\F \\circ (\\Vmh \\gVm)$\n",
    "\n",
    "$\\Kc = \\I \\circ (\\Vmh \\gVm) $\n",
    "\n",
    "$ \\Vm $ is the left most $m$ columns of $\\V $.\n",
    "\n",
    "$ \\gVm $ is the left most $m$ columns of $\\gV $.\n",
    "\n",
    "$ \\Vr $ is the right most $n - m$ columns of $\\V$.\n",
    "\n",
    "$ \\gVr $ is the right most $n - m$ columns of $\\gV$.\n",
    "\n",
    "\n",
    "### Forward mode\n",
    "\n",
    "The reference does not explicitly document the differentials for forward mode. It is possible that they are parseable out of the reference, but I'm skipping the complex forward mode for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89524739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _H(x): return jnp.conj(x.T)\n",
    "\n",
    "def svd_vjp_complex_valued_full(vals, grads):\n",
    "    U, S_vals, Vh = vals\n",
    "    gU, gS_vals, gVh = grads\n",
    "    \n",
    "    m = U.shape[0]\n",
    "    n = Vh.shape[0]\n",
    "    \n",
    "    if m > n:\n",
    "        gA = svd_vjp_complex_valued_full((_H(Vh), S_vals, _H(U)), (_H(gVh), gS_vals, _H(gU)))\n",
    "        return _H(gA)\n",
    "    \n",
    "    # m <= n\n",
    "    \n",
    "    S = fill_diagonal(jnp.zeros((m, n)), S_vals)\n",
    "    gS = fill_diagonal(jnp.zeros((m, n)), gS_vals)\n",
    "    S_1 = fill_diagonal(jnp.zeros((m, m)), S_vals)\n",
    "    S_1h = _H(S_1)\n",
    "    S_1_inv = jnp.linalg.inv(S_1)\n",
    "    \n",
    "    F_ = F(S_vals, m)\n",
    "    Fh = _H(F_)\n",
    "    \n",
    "    V = _H(Vh)\n",
    "    Uh = _H(U)\n",
    "    gSh = _H(gS)\n",
    "    gV = _H(gVh)\n",
    "    \n",
    "    Vm = V[:,:m]\n",
    "    Vr = V[:,m:]\n",
    "    Vmh = _H(Vm)\n",
    "    Vrh = _H(Vr)\n",
    "    \n",
    "    gVm = gV[:,:m]\n",
    "    gVr = gV[:,m:]\n",
    "    gVmh = _H(gVm)\n",
    "    \n",
    "    \n",
    "    ############ term_U ##########\n",
    "    \n",
    "    K_u = F_ * (Uh @ gU)\n",
    "    K_uh = _H(K_u)\n",
    "    term_U = U @ (K_u + K_uh) @ S_1 @ Vmh\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ########## term_S ###########\n",
    "    \n",
    "    I = fill_diagonal(jnp.zeros((m, n)), 1)\n",
    "    term_S = U @ (I * gS) @ Vh\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ######### term_V ############\n",
    "\n",
    "    K_v = F_ * (Vmh @ gVm)\n",
    "    K_vh = _H(K_v)\n",
    "    \n",
    "    term_V1 = U @ S_1h @ (K_v + K_vh) @ Vmh\n",
    "    term_V2 = U @ S_1_inv @ (gVmh @ Vr - Vmh @ gVr) @ Vrh\n",
    "    \n",
    "    term_V = term_V1 + term_V2\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    ########### term_C ###########\n",
    "    \n",
    "    K_c = jnp.eye(m) * (Vmh @ gVm)\n",
    "    K_ch = _H(K_c)\n",
    "    \n",
    "    term_C = 0.5 * U @ S_1_inv @ (K_ch - K_c) @ Vmh\n",
    "    \n",
    "    ############################\n",
    "    \n",
    "    gA = term_U + term_S + term_V + term_C\n",
    "    \n",
    "    return (gA,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af016b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto diff:\n",
      " [[-0.05428613+0.03160932j -0.12564869+0.10420159j]\n",
      " [-0.23833506-0.1511732j  -0.27746832-0.05487967j]]\n",
      "numerical:\n",
      " [[ 0.04470348-0.14156103j  0.05215406+0.09685755j]\n",
      " [-0.09685755-0.17881393j  0.28312206+0.20116568j]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(False, dtype=bool)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.custom_vjp\n",
    "def svd_complex_valued_full(A):\n",
    "    return jnp.linalg.svd(A, full_matrices=True, compute_uv=True)\n",
    "\n",
    "# Dummy jvp, not actual jvp\n",
    "def svd_jvp_complex_valued_full(A):\n",
    "    u, s, v = svd_complex_valued_full(A)\n",
    "    return (u, s, v), (u, s, v)\n",
    "\n",
    "svd_complex_valued_full.defvjp(svd_jvp_complex_valued_full, svd_vjp_complex_valued_full)\n",
    "\n",
    "def l(A):\n",
    "    u, s, v = svd_complex_valued_full(A)\n",
    "    return jnp.real(u[0, -1] * v[-1, 0])\n",
    "\n",
    "# NOTE(will) - It's hard to get equal SVD values for complex inputs between\n",
    "# jax and tf. Instead, check against numerical solution\n",
    "# https://github.com/Zhouquan-Wan/SVD_autodiff/blob/master/svd_ad_jax.py\n",
    "def check_complex_valued_full_vjp(key, m, n):\n",
    "    k1, k2 = random.split(key)\n",
    "    \n",
    "    Ax = random.normal(k1, (m, n))\n",
    "    Ay = random.normal(k2, (m, n))\n",
    "    \n",
    "    A = jnp.array(Ax + 1.0j * Ay, dtype='complex64')\n",
    "    \n",
    "    # auto diff\n",
    "    DA_ad = jax.grad(l)(A)\n",
    "    print(\"auto diff:\\n\", DA_ad)\n",
    "    \n",
    "    # numerical\n",
    "    d = 1e-6\n",
    "    DA = jnp.zeros((m, n), dtype='complex64')\n",
    "    for i in range(0, m):\n",
    "        for j in range(0, n):\n",
    "            dA = jnp.zeros((m, n))\n",
    "            dA = dA.at[i, j].set(1)\n",
    "            DA = DA.at[i, j].set((l(A + d * dA) - l(A)) / d - 1.0j * (\n",
    "                l(A + d * 1.0j * dA) - l(A)\n",
    "            ) / d)\n",
    "            \n",
    "    print(\"numerical:\\n\", DA)\n",
    "    \n",
    "    return check_allclose(DA, DA_ad)\n",
    "\n",
    "def check_complex_valued_fulls_vjp():\n",
    "    key = random.PRNGKey(0)\n",
    "    \n",
    "    for m in range(2, 11):\n",
    "        # gradient known bad if abs(n - m) >= 2, so constrain range of dimensions\n",
    "        min_n = max(m - 1, 2)\n",
    "        max_n = min(m + 2, 11)\n",
    "        for n in range(min_n, max_n):\n",
    "            assert(abs(m - n) < 2)\n",
    "            key, subkey = random.split(key)\n",
    "            return check_complex_valued_full_vjp(subkey, m, n)\n",
    "        \n",
    "check_complex_valued_fulls_vjp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72312621",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "SVD real differentiability:\n",
    "- https://j-towns.github.io/papers/svd-derivative.pdf\n",
    "- https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n",
    "- https://arxiv.org/pdf/1509.07838.pdf\n",
    "\n",
    "SVD complex differentiability:\n",
    "- https://arxiv.org/pdf/1909.02659.pdf\n",
    "- https://giggleliu.github.io/2019/04/02/einsumbp.html\n",
    "\n",
    "Existing implementations:\n",
    "\n",
    "Jax forward:\n",
    "- https://github.com/google/jax/blob/2a00533e3e686c1c9d7dfe9ed2a3b19217cfe76f/jax/_src/lax/linalg.py#L1578\n",
    "- Jax only implements the forward rule because jax can derive the backward rule from the forward rule and vice versa.\n",
    "\n",
    "Pytorch forward:\n",
    "- https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3122\n",
    "\n",
    "Pytorch backward:\n",
    "- https://github.com/pytorch/pytorch/blob/7a8152530d490b30a56bb090e9a67397d20e16b1/torch/csrc/autograd/FunctionsManual.cpp#L3228\n",
    "\n",
    "Tensorflow backward:\n",
    "- https://github.com/tensorflow/tensorflow/blob/bbe41abdcb2f7e923489bfa21cfb546b6022f330/tensorflow/python/ops/linalg_grad.py#L815\n",
    "\n",
    "General complex differentiability:\n",
    "- https://mediatum.ub.tum.de/doc/631019/631019.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
